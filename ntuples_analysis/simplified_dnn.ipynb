{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Radeon 890M (RDNA 3.5 / gfx1150) cannot be used with Pytorch\n",
    "# os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"11.0.2\"\n",
    "\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support,\n",
    "    balanced_accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    brier_score_loss,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "VERBOSE = False\n",
    "FULL_TRAINING = False\n",
    "\n",
    "main_branch = \"Events\"\n",
    "tk_branches = [\n",
    "    \"muon_pixel_tracks_p\",\n",
    "    \"muon_pixel_tracks_pt\",\n",
    "    \"muon_pixel_tracks_ptErr\",\n",
    "    \"muon_pixel_tracks_eta\",\n",
    "    \"muon_pixel_tracks_etaErr\",\n",
    "    \"muon_pixel_tracks_phi\",\n",
    "    \"muon_pixel_tracks_phiErr\",\n",
    "    \"muon_pixel_tracks_chi2\",\n",
    "    \"muon_pixel_tracks_normalizedChi2\",\n",
    "    \"muon_pixel_tracks_nPixelHits\",\n",
    "    \"muon_pixel_tracks_nTrkLays\",\n",
    "    \"muon_pixel_tracks_nFoundHits\",\n",
    "    \"muon_pixel_tracks_nLostHits\",\n",
    "    \"muon_pixel_tracks_dsz\",\n",
    "    \"muon_pixel_tracks_dszErr\",\n",
    "    \"muon_pixel_tracks_dxy\",\n",
    "    \"muon_pixel_tracks_dxyErr\",\n",
    "    \"muon_pixel_tracks_dz\",\n",
    "    \"muon_pixel_tracks_dzErr\",\n",
    "    \"muon_pixel_tracks_qoverp\",\n",
    "    \"muon_pixel_tracks_qoverpErr\",\n",
    "    \"muon_pixel_tracks_lambdaErr\",\n",
    "    \"muon_pixel_tracks_matched\",\n",
    "    \"muon_pixel_tracks_duplicate\",\n",
    "    \"muon_pixel_tracks_tpPdgId\",\n",
    "    \"muon_pixel_tracks_tpPt\",\n",
    "    \"muon_pixel_tracks_tpEta\",\n",
    "    \"muon_pixel_tracks_tpPhi\",\n",
    "]\n",
    "\n",
    "l1tkMuon_branches = [\n",
    "    \"L1TkMu_pt\",\n",
    "    \"L1TkMu_eta\",\n",
    "    \"L1TkMu_phi\",\n",
    "]\n",
    "\n",
    "# Input files\n",
    "files = []\n",
    "data_dir = \"data/train\"\n",
    "i = 0\n",
    "for path in os.listdir(data_dir):\n",
    "    if os.path.isfile(os.path.join(data_dir, path)) and \"gun\" in path:\n",
    "        if i == 1:\n",
    "            files.append(os.path.join(data_dir, path))\n",
    "        i += 1\n",
    "\n",
    "print(f\"Selected {len(files)} input files:\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73910047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    condition = eval(line)\n",
    "    if not condition:\n",
    "        print(\"Skipping cell...\")\n",
    "    else:\n",
    "        exec(cell, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6040ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "\"\"\"\n",
    "import random\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\"\"\"\n",
    "use_gpu = False  # RDNA 3.5 (890M) is currently unstable\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Torch version HIP:\", torch.version.hip)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Diagnostics for AMD Radeon 890M (RDNA 3.5)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"AMD GPU DIAGNOSTICS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 1. Basic Properties\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"VRAM (Total): {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Compute Capability: {props.major}.{props.minor}\")\n",
    "\n",
    "        # 2. Allocation Test\n",
    "        print(\"\\n[Test 1] Memory Allocation...\")\n",
    "        x = torch.ones(1024, 1024, device=device)\n",
    "        print(\"Success\")\n",
    "\n",
    "        # 3. Computation Test\n",
    "        print(\"\\n[Test 2] Matrix Multiplication...\")\n",
    "        y = torch.matmul(x, x)\n",
    "        print(\"Success\")\n",
    "\n",
    "        # 4. Stream Test\n",
    "        print(\"\\n[Test 3] CUDA Streams...\")\n",
    "        s = torch.cuda.Stream()\n",
    "        with torch.cuda.stream(s):\n",
    "            z = torch.matmul(y, y)\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"Success\")\n",
    "\n",
    "        print(\"\\nGPU seems functional\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nGPU FAILED: {e}\")\n",
    "        print(\n",
    "            \"Current Override: HSA_OVERRIDE_GFX_VERSION =\",\n",
    "            os.environ.get(\"HSA_OVERRIDE_GFX_VERSION\", \"Not Set\"),\n",
    "        )\n",
    "        print(\"Try changing the override in the first cell:\")\n",
    "        print(\"os.environ['HSA_OVERRIDE_GFX_VERSION'] = '11.0.2'\")\n",
    "else:\n",
    "    print(\n",
    "        \"Running on CPU. Integrated graphics not supported on PyTorch version: 2.10.0.dev20251210+rocm7.1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b0930",
   "metadata": {},
   "source": [
    "## Build Feature Matrix and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"muon_pixel_tracks_p\",\n",
    "    \"muon_pixel_tracks_pt\",\n",
    "    \"muon_pixel_tracks_ptErr\",\n",
    "    \"muon_pixel_tracks_eta\",\n",
    "    \"muon_pixel_tracks_etaErr\",\n",
    "    \"muon_pixel_tracks_phi\",\n",
    "    \"muon_pixel_tracks_phiErr\",\n",
    "    \"muon_pixel_tracks_chi2\",\n",
    "    \"muon_pixel_tracks_normalizedChi2\",\n",
    "    \"muon_pixel_tracks_nPixelHits\",\n",
    "    \"muon_pixel_tracks_nTrkLays\",\n",
    "    \"muon_pixel_tracks_nFoundHits\",\n",
    "    \"muon_pixel_tracks_nLostHits\",\n",
    "    \"muon_pixel_tracks_dsz\",\n",
    "    \"muon_pixel_tracks_dszErr\",\n",
    "    \"muon_pixel_tracks_dxy\",\n",
    "    \"muon_pixel_tracks_dxyErr\",\n",
    "    \"muon_pixel_tracks_dz\",\n",
    "    \"muon_pixel_tracks_dzErr\",\n",
    "    \"muon_pixel_tracks_qoverp\",\n",
    "    \"muon_pixel_tracks_qoverpErr\",\n",
    "    \"muon_pixel_tracks_lambdaErr\",\n",
    "]\n",
    "\n",
    "LABEL_FIELD = \"muon_pixel_tracks_matched\"\n",
    "useL1TkMuFeatures = True\n",
    "\n",
    "\n",
    "def wrap_phi(phi):\n",
    "    \"\"\"Wrap phi to [-pi, pi]\"\"\"\n",
    "    return ((phi + np.pi) % (2 * np.pi)) - np.pi\n",
    "\n",
    "\n",
    "def build_dataset(arr, file_labels_in):\n",
    "    print(\"Building dataset...\")\n",
    "    feature_names = list(features)\n",
    "    mask = arr[feature_names[0]] >= 0\n",
    "\n",
    "    # Expand file labels from event level to track level\n",
    "    n_tracks_per_event = ak.num(arr[feature_names[0]])\n",
    "    file_labels_jagged = ak.unflatten(\n",
    "        np.repeat(file_labels_in, n_tracks_per_event), n_tracks_per_event\n",
    "    )\n",
    "    file_labels_masked = ak.to_numpy(ak.flatten(file_labels_jagged[mask]))\n",
    "\n",
    "    # Store original kinematics before log transformation\n",
    "    trk_pt_original = arr[\"muon_pixel_tracks_pt\"]\n",
    "    trk_ptErr_original = arr[\"muon_pixel_tracks_ptErr\"]\n",
    "    trk_eta_original = arr[\"muon_pixel_tracks_eta\"]\n",
    "    trk_phi_original = arr[\"muon_pixel_tracks_phi\"]\n",
    "    trk_chi2 = arr[\"muon_pixel_tracks_chi2\"]\n",
    "    trk_nFoundHits = arr[\"muon_pixel_tracks_nFoundHits\"]\n",
    "    trk_nLostHits = arr[\"muon_pixel_tracks_nLostHits\"]\n",
    "    trk_dxy = arr[\"muon_pixel_tracks_dxy\"]\n",
    "    trk_dz = arr[\"muon_pixel_tracks_dz\"]\n",
    "    trk_dxyErr = arr[\"muon_pixel_tracks_dxyErr\"]\n",
    "    trk_dzErr = arr[\"muon_pixel_tracks_dzErr\"]\n",
    "    trk_qoverp_original = arr[\"muon_pixel_tracks_qoverp\"]\n",
    "    trk_qoverpErr_original = arr[\"muon_pixel_tracks_qoverpErr\"]\n",
    "\n",
    "    cols = []\n",
    "    for f in features:\n",
    "        minimum = ak.min(ak.flatten(arr[f][mask]))\n",
    "        maximum = ak.max(ak.flatten(arr[f][mask]))\n",
    "        if f in [\"muon_pixel_tracks_p\", \"muon_pixel_tracks_pt\"] or \"Err\" in f:\n",
    "            if VERBOSE:\n",
    "                print(f\"Feature {f} min {minimum:.2f} max {maximum:.2f} -> log10\")\n",
    "            arr[f] = np.log10(arr[f] + 1e-6)\n",
    "        flat = ak.to_numpy(ak.flatten(arr[f][mask])).astype(np.float32)\n",
    "        cols.append(flat)\n",
    "\n",
    "    # DERIVED FEATURES - Help model generalize across physics processes\n",
    "    print(\"\\nAdding derived features...\")\n",
    "\n",
    "    # 1. Relative momentum uncertainty (already have this)\n",
    "    sigmaPtOverPt = np.log10(trk_ptErr_original / trk_pt_original + 1e-6)\n",
    "    cols.append(ak.to_numpy(ak.flatten(sigmaPtOverPt[mask])).astype(np.float32))\n",
    "    feature_names.append(\"muon_pixel_tracks_sigmaPtOverPt\")\n",
    "\n",
    "    # 2. Hit efficiency: found / (found + lost)\n",
    "    hit_efficiency = trk_nFoundHits / (trk_nFoundHits + trk_nLostHits)\n",
    "    cols.append(ak.to_numpy(ak.flatten(hit_efficiency[mask])).astype(np.float32))\n",
    "    feature_names.append(\"muon_pixel_tracks_hitEfficiency\")\n",
    "\n",
    "    # 3. Chi2 per hit (quality per measurement)\n",
    "    chi2_per_hit = trk_chi2 / trk_nFoundHits\n",
    "    cols.append(\n",
    "        ak.to_numpy(ak.flatten(np.log10(chi2_per_hit + 1e-6)[mask])).astype(np.float32)\n",
    "    )\n",
    "    feature_names.append(\"muon_pixel_tracks_chi2PerHit\")\n",
    "\n",
    "    # 4. Impact parameter significance (3D)\n",
    "    impact_param_3d = trk_dxy**2 + trk_dz**2\n",
    "    cols.append(\n",
    "        ak.to_numpy(ak.flatten(np.log10(impact_param_3d + 1e-6)[mask])).astype(\n",
    "            np.float32\n",
    "        )\n",
    "    )\n",
    "    feature_names.append(\"muon_pixel_tracks_impact3D\")\n",
    "\n",
    "    # 5. Impact parameter significance (normalized by uncertainty)\n",
    "    dxy_significance = trk_dxy / trk_dxyErr\n",
    "    dz_significance = trk_dz / trk_dzErr\n",
    "    impact_significance_2d = np.sqrt(dxy_significance**2 + dz_significance**2)\n",
    "    cols.append(\n",
    "        ak.to_numpy(ak.flatten(np.log10(impact_significance_2d + 1e-6)[mask])).astype(\n",
    "            np.float32\n",
    "        )\n",
    "    )\n",
    "    feature_names.append(\"muon_pixel_tracks_impactSignificance\")\n",
    "\n",
    "    # 6. Relative uncertainties product (captures overall measurement quality)\n",
    "    rel_uncertainty_product = (trk_ptErr_original / trk_pt_original) * (\n",
    "        trk_qoverpErr_original / trk_qoverp_original\n",
    "    )\n",
    "    cols.append(\n",
    "        ak.to_numpy(ak.flatten(np.log10(rel_uncertainty_product + 1e-6)[mask])).astype(\n",
    "            np.float32\n",
    "        )\n",
    "    )\n",
    "    feature_names.append(\"muon_pixel_tracks_relUncertaintyProduct\")\n",
    "\n",
    "    if useL1TkMuFeatures:\n",
    "        print(\"\\nComputing L1TkMuon matching features...\")\n",
    "\n",
    "        l1_pt = arr[\"L1TkMu_pt\"]\n",
    "        l1_eta = arr[\"L1TkMu_eta\"]\n",
    "        l1_phi = arr[\"L1TkMu_phi\"]\n",
    "\n",
    "        trk_zip = ak.zip(\n",
    "            {\"pt\": trk_pt_original, \"eta\": trk_eta_original, \"phi\": trk_phi_original}\n",
    "        )\n",
    "        l1_zip = ak.zip({\"pt\": l1_pt, \"eta\": l1_eta, \"phi\": l1_phi})\n",
    "\n",
    "        pairs = ak.cartesian({\"t\": trk_zip, \"l\": l1_zip}, axis=1, nested=True)\n",
    "\n",
    "        deta = pairs.t.eta - pairs.l.eta\n",
    "        dphi = wrap_phi(pairs.t.phi - pairs.l.phi)\n",
    "        dR2 = deta**2 + dphi**2\n",
    "\n",
    "        min_idx = ak.argmin(dR2, axis=2)\n",
    "        dR2_min = ak.firsts(\n",
    "            dR2[ak.local_index(dR2, axis=2) == min_idx[..., None]], axis=2\n",
    "        )\n",
    "        l1_pt_matched = ak.firsts(\n",
    "            pairs.l.pt[ak.local_index(pairs.l.pt, axis=2) == min_idx[..., None]], axis=2\n",
    "        )\n",
    "\n",
    "        dPt_norm = np.abs(trk_pt_original - l1_pt_matched) / l1_pt_matched\n",
    "\n",
    "        # L1 pT ratio\n",
    "        pt_ratio = trk_pt_original / l1_pt_matched\n",
    "\n",
    "        # Combined matching score (dR and pT)\n",
    "        matching_score = dR2_min * (1 + dPt_norm)\n",
    "\n",
    "        dR2_min = ak.fill_none(dR2_min, 999.0)\n",
    "        dPt_norm = ak.fill_none(dPt_norm, 999.0)\n",
    "        pt_ratio = ak.fill_none(pt_ratio, 999.0)\n",
    "        matching_score = ak.fill_none(matching_score, 999.0)\n",
    "\n",
    "        dR2_min_flat = ak.to_numpy(ak.flatten(dR2_min[mask]))\n",
    "        dPt_norm_flat = ak.to_numpy(ak.flatten(dPt_norm[mask]))\n",
    "        pt_ratio_flat = ak.to_numpy(ak.flatten(pt_ratio[mask]))\n",
    "        matching_score_flat = ak.to_numpy(ak.flatten(matching_score[mask]))\n",
    "\n",
    "        valid_matches = dPt_norm_flat < 999.0\n",
    "\n",
    "        if VERBOSE:\n",
    "            print(\n",
    "                f\"  ΔR2_min: min={dR2_min_flat.min():.4f}, max={dR2_min_flat.max():.4f}, mean={dR2_min_flat.mean():.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  ΔpT_norm: min={dPt_norm_flat.min():.4f}, max={dPt_norm_flat.max():.4f}, mean={dPt_norm_flat.mean():.4f}\"\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"  Valid L1TkMu matches: {valid_matches.sum()}/{len(valid_matches)} ({valid_matches.mean() * 100:.2f}%)\"\n",
    "            )\n",
    "            if valid_matches.any():\n",
    "                print(\n",
    "                    f\"  ΔpT_norm (valid only): mean={dPt_norm_flat[valid_matches].mean():.4f}, \"\n",
    "                    f\"median={np.median(dPt_norm_flat[valid_matches]):.4f}, \"\n",
    "                    f\"95th percentile={np.percentile(dPt_norm_flat[valid_matches], 95):.4f}\"\n",
    "                )\n",
    "\n",
    "        cols.append(np.log10(dR2_min_flat + 1e-6).astype(np.float32))\n",
    "        cols.append(np.log10(dPt_norm_flat + 1e-6).astype(np.float32))\n",
    "        cols.append(np.log10(pt_ratio_flat + 1e-6).astype(np.float32))\n",
    "        cols.append(np.log10(matching_score_flat + 1e-6).astype(np.float32))\n",
    "\n",
    "        feature_names.append(\"L1TkMu_dR2min\")\n",
    "        feature_names.append(\"L1TkMu_dPtNorm\")\n",
    "        feature_names.append(\"L1TkMu_ptRatio\")\n",
    "        feature_names.append(\"L1TkMu_matchingScore\")\n",
    "\n",
    "    X = np.vstack(cols).T.astype(np.float32)\n",
    "    y = ak.to_numpy(ak.flatten(arr[LABEL_FIELD][mask])).astype(np.int8)\n",
    "\n",
    "    finite = np.isfinite(X).all(axis=1)\n",
    "    if not finite.all():\n",
    "        print(f\"Removing {(~finite).sum()} non-finite samples\")\n",
    "        X = X[finite]\n",
    "        y = y[finite]\n",
    "        file_labels_masked = file_labels_masked[finite]\n",
    "\n",
    "    return X, y, file_labels_masked, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4889362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental loading and processing to save memory\n",
    "import gc\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "file_labels_list = []\n",
    "feature_names = []\n",
    "total_events = 0\n",
    "\n",
    "print(f\"Processing {len(files)} files incrementally...\")\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    print(f\"[{i + 1}/{len(files)}] Loading {f}...\")\n",
    "    with uproot.open(f) as file:\n",
    "        arrays_f = file[main_branch].arrays(tk_branches + l1tkMuon_branches)\n",
    "        n_events = len(arrays_f)\n",
    "        total_events += n_events\n",
    "\n",
    "        # Create temporary file labels\n",
    "        file_labels_temp = np.full(n_events, i)\n",
    "\n",
    "        X_chunk, y_chunk, labels_chunk, feats = build_dataset(\n",
    "            arrays_f, file_labels_temp\n",
    "        )\n",
    "\n",
    "        X_list.append(X_chunk)\n",
    "        y_list.append(y_chunk)\n",
    "        file_labels_list.append(labels_chunk)\n",
    "\n",
    "        if i == 0:\n",
    "            feature_names = feats\n",
    "\n",
    "        # Free memory immediately\n",
    "        del arrays_f\n",
    "        del X_chunk, y_chunk, labels_chunk\n",
    "        gc.collect()\n",
    "        print(f\"Loaded {n_events} events from {f}\")\n",
    "\n",
    "X = np.concatenate(X_list)\n",
    "y = np.concatenate(y_list)\n",
    "file_labels_flat = np.concatenate(file_labels_list)\n",
    "\n",
    "print(f\"Loaded {total_events} events from {len(files)} files\")\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape, \"Positives:\", y.sum(), f\"({100 * y.mean():.2f}%)\")\n",
    "print(\"Feature order:\", feature_names)\n",
    "\n",
    "# Show distribution across files\n",
    "print(\"\\nSample distribution by file:\")\n",
    "for file_idx, fname in enumerate(files):\n",
    "    n_samples = (file_labels_flat == file_idx).sum()\n",
    "    n_pos = ((file_labels_flat == file_idx) & (y == 1)).sum()\n",
    "    print(\n",
    "        f\"  File {file_idx} ({fname.split('/')[-1]}): {n_samples} samples ({n_pos} positive, {100 * n_pos / max(n_samples, 1):.2f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b55125",
   "metadata": {},
   "source": [
    "## Train/Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe891ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.7  # fraction for train+test\n",
    "val_frac = 0.15  # fraction (of full dataset) reserved for validation (inside train+val)\n",
    "\n",
    "# Create composite stratification label: combine class label + file source\n",
    "# This ensures both class balance AND file representation in each split\n",
    "stratify_label = (\n",
    "    y * len(files) + file_labels_flat\n",
    ")  # Unique label per (class, file) combination\n",
    "\n",
    "# First split train_val vs test\n",
    "X_train_val, X_test, y_train_val, y_test, file_train_val, file_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    file_labels_flat,\n",
    "    train_size=train_frac,\n",
    "    stratify=stratify_label if (y.sum() > 0 and y.sum() < len(y)) else None,\n",
    ")\n",
    "\n",
    "# Derive actual validation fraction relative to train_val portion\n",
    "if val_frac > 0:\n",
    "    rel_val = val_frac / train_frac  # portion of train_val to carve out as validation\n",
    "    stratify_train_val = y_train_val * len(files) + file_train_val\n",
    "    X_train, X_val, y_train, y_val, file_train, file_val = train_test_split(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        file_train_val,\n",
    "        test_size=rel_val,\n",
    "        stratify=stratify_train_val\n",
    "        if (y_train_val.sum() > 0 and y_train_val.sum() < len(y_train_val))\n",
    "        else None,\n",
    "    )\n",
    "else:\n",
    "    X_train, y_train, file_train = X_train_val, y_train_val, file_train_val\n",
    "    X_val, y_val, file_val = X_test, y_test, file_test\n",
    "\n",
    "print(\n",
    "    \"Train size:\", X_train.shape, \"Val size:\", X_val.shape, \"Test size:\", X_test.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if VERBOSE\n",
    "# Examine features pre-scaling\n",
    "print(\"Pre-scaling feature stats:\")\n",
    "for i in range(X_train.shape[1]):\n",
    "    print(\n",
    "        f\"{feature_names[i].split('_')[-1]}: mean={X_train[:, i].mean():.4f}, std={X_train[:, i].std():.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51555a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "if VERBOSE:\n",
    "    print(\"\\nPost-scaling feature stats:\")\n",
    "    for i in range(X_train.shape[1]):\n",
    "        print(\n",
    "            f\"{feature_names[i].split('_')[-1]}: mean={X_train[:, i].mean():.4f}, std={X_train[:, i].std():.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e699a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify file distribution in each split\n",
    "print(\"\\nFile Distribution Verification\")\n",
    "for split_name, file_split, y_split in [\n",
    "    (\"Train\", file_train, y_train),\n",
    "    (\"Val\", file_val, y_val),\n",
    "    (\"Test\", file_test, y_test),\n",
    "]:\n",
    "    print(f\"\\n{split_name} set:\")\n",
    "    for file_idx, fname in enumerate(files):\n",
    "        n_samples = (file_split == file_idx).sum()\n",
    "        n_pos = ((file_split == file_idx) & (y_split == 1)).sum()\n",
    "        pct_of_split = 100 * n_samples / len(file_split)\n",
    "        print(\n",
    "            f\"  {fname.split('/')[-1]}: {n_samples} samples ({pct_of_split:.1f}% of {split_name.lower()}, {n_pos} pos)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461dfdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLIFIED CLASS BALANCING - Choose one strategy\n",
    "use_undersampling = False  # Try turning OFF undersampling\n",
    "use_weighted_loss = True  # Use class weights instead\n",
    "use_focal = False  # Don't use focal loss with pos_weight\n",
    "\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32)).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Simple random undersampling without file complexity\n",
    "if use_undersampling:\n",
    "    pos_indices = np.where(y_train == 1)[0]\n",
    "    neg_indices = np.where(y_train == 0)[0]\n",
    "\n",
    "    # Keep all positives, undersample negatives to 3:1 ratio\n",
    "    n_neg_keep = min(len(neg_indices), len(pos_indices) * 3)\n",
    "    neg_indices_sampled = np.random.choice(neg_indices, size=n_neg_keep, replace=False)\n",
    "\n",
    "    balanced_indices = np.concatenate([pos_indices, neg_indices_sampled])\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    X_train_balanced = X_train[balanced_indices]\n",
    "    y_train_balanced = y_train[balanced_indices]\n",
    "\n",
    "    print(f\"Undersampling: {len(y_train)} -> {len(y_train_balanced)} samples\")\n",
    "    print(f\"  Pos: {y_train_balanced.sum()} ({100 * y_train_balanced.mean():.1f}%)\")\n",
    "\n",
    "    train_ds = NumpyDataset(X_train_balanced, y_train_balanced)\n",
    "    pos_count = y_train_balanced.sum()\n",
    "    neg_count = len(y_train_balanced) - pos_count\n",
    "else:\n",
    "    train_ds = NumpyDataset(X_train, y_train)\n",
    "    pos_count = y_train.sum()\n",
    "    neg_count = len(y_train) - pos_count\n",
    "\n",
    "# Compute pos_weight for BCE loss\n",
    "if use_weighted_loss and pos_count > 0:\n",
    "    pos_weight_value = neg_count / pos_count\n",
    "    # Don't multiply - let the ratio speak for itself\n",
    "    pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32, device=device)\n",
    "    print(f\"Using pos_weight={pos_weight.item():.2f}\")\n",
    "else:\n",
    "    pos_weight = None\n",
    "    print(\"Using unweighted loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6a60b",
   "metadata": {},
   "source": [
    "## Torch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "\n",
    "val_ds = NumpyDataset(X_val, y_val)\n",
    "test_ds = NumpyDataset(X_test, y_test)\n",
    "\n",
    "# if not use_undersampling:\n",
    "#    # inverse frequency sampling to upweight minority\n",
    "#    class_sample_counts = np.array([(y_train == 0).sum(), (y_train == 1).sum()])\n",
    "#    weights = 1.0 / np.clip(class_sample_counts, 1, None)\n",
    "#    sample_weights = weights[y_train]\n",
    "#    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "#        sample_weights, num_samples=len(sample_weights), replacement=True\n",
    "#    )\n",
    "#    train_loader = DataLoader(\n",
    "#        train_ds, batch_size=batch_size, sampler=sampler, drop_last=False\n",
    "#    )\n",
    "# else:\n",
    "#    train_loader = DataLoader(\n",
    "#        train_ds, batch_size=batch_size, shuffle=True, drop_last=False#\n",
    "#  )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"First batch shapes:\", xb.shape, yb.shape)\n",
    "print(\n",
    "    \"Class balance train: pos=\",\n",
    "    (y_train_balanced if use_undersampling else y_train).sum(),\n",
    "    \"neg=\",\n",
    "    len(y_train_balanced if use_undersampling else y_train)\n",
    "    - (y_train_balanced if use_undersampling else y_train).sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7da74",
   "metadata": {},
   "source": [
    "## Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655da510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Training Random Forest for feature importance analysis...\")\n",
    "# Use a subset for faster training\n",
    "sample_size = min(1000000, len(X_train))\n",
    "sample_idx = np.random.choice(len(X_train), size=sample_size, replace=False)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=10, min_samples_leaf=50, n_jobs=-1, random_state=42\n",
    ")\n",
    "rf.fit(X_train[sample_idx], y_train[sample_idx])\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE IMPORTANCE RANKING (Random Forest)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "importance_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Feature\": [feature_names[i] for i in indices],\n",
    "        \"Importance\": importances[indices],\n",
    "        \"Cumulative\": np.cumsum(importances[indices]),\n",
    "    }\n",
    ")\n",
    "\n",
    "for idx, row in importance_df.iterrows():\n",
    "    feat_name = row[\"Feature\"].split(\"_\")[-1]\n",
    "    print(\n",
    "        f\"{idx + 1:2d}. {feat_name:20s}: {row['Importance']:.4f} (cumulative: {row['Cumulative']:.4f})\"\n",
    "    )\n",
    "\n",
    "# Find features contributing to 98% of importance\n",
    "threshold_98 = 0.98\n",
    "important_features_98 = importance_df[importance_df[\"Cumulative\"] <= threshold_98][\n",
    "    \"Feature\"\n",
    "].tolist()\n",
    "# Add one more feature to cross the threshold\n",
    "if len(important_features_98) < len(feature_names):\n",
    "    important_features_98.append(\n",
    "        importance_df.iloc[len(important_features_98)][\"Feature\"]\n",
    "    )\n",
    "\n",
    "print(f\"\\n{len(important_features_98)} features explain 98% of importance\")\n",
    "print(\"Top features:\", [f.split(\"_\")[-1] for f in important_features_98[:10]])\n",
    "\n",
    "# Permutation importance (more reliable but slower)\n",
    "print(\n",
    "    \"\\nComputing permutation importance on validation set (this may take a minute)...\"\n",
    ")\n",
    "perm_importance = permutation_importance(\n",
    "    rf, X_val[:10000], y_val[:10000], n_repeats=5, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "perm_indices = np.argsort(perm_importance.importances_mean)[::-1]\n",
    "print(\"\\nTop 10 by Permutation Importance:\")\n",
    "for i in range(min(10, len(perm_indices))):\n",
    "    idx = perm_indices[i]\n",
    "    feat_name = feature_names[idx].split(\"_\")[-1]\n",
    "    print(\n",
    "        f\"  {feat_name:20s}: {perm_importance.importances_mean[idx]:.4f} ± {perm_importance.importances_std[idx]:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Bar chart of top features\n",
    "ax = axes[0]\n",
    "top_n = 15\n",
    "top_features = [feature_names[i].split(\"_\")[-1] for i in indices[:top_n]]\n",
    "top_importance = importances[indices[:top_n]]\n",
    "ax.barh(range(top_n), top_importance[::-1])\n",
    "ax.set_yticks(range(top_n))\n",
    "ax.set_yticklabels(top_features[::-1])\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_title(f\"Top {top_n} Features by Importance\")\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Plot 2: Cumulative importance\n",
    "ax = axes[1]\n",
    "ax.plot(\n",
    "    range(1, len(importances) + 1),\n",
    "    np.cumsum(importances[indices]),\n",
    "    marker=\"o\",\n",
    "    markersize=4,\n",
    ")\n",
    "ax.axhline(y=0.98, color=\"r\", linestyle=\"--\", label=\"98% threshold\")\n",
    "ax.axvline(x=len(important_features_98), color=\"r\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set_xlabel(\"Number of Features\")\n",
    "ax.set_ylabel(\"Cumulative Importance\")\n",
    "ax.set_title(\"Cumulative Feature Importance\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_output/feature_importance.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Feature analysis complete. Consider using top {len(important_features_98)} features.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e300bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use top N features by importance\n",
    "use_feature_pruning = True\n",
    "n_features_to_keep = (\n",
    "    min(20, len(important_features_98) + 4)\n",
    "    if len(important_features_98) < 20\n",
    "    else len(important_features_98)\n",
    ")\n",
    "if use_feature_pruning:\n",
    "    # Select top features\n",
    "    selected_feature_indices = indices[:n_features_to_keep]\n",
    "    selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "    print(f\"Pruning features: {len(feature_names)} -> {len(selected_feature_names)}\")\n",
    "    print(\"Selected features:\", [f.split(\"_\")[-1] for f in selected_feature_names])\n",
    "\n",
    "    # Create pruned datasets\n",
    "    X_train_pruned = X_train[:, selected_feature_indices]\n",
    "    X_val_pruned = X_val[:, selected_feature_indices]\n",
    "    X_test_pruned = X_test[:, selected_feature_indices]\n",
    "\n",
    "    # Update for model training\n",
    "    X_train_model = X_train_pruned\n",
    "    X_val_model = X_val_pruned\n",
    "    X_test_model = X_test_pruned\n",
    "    feature_names_model = selected_feature_names\n",
    "\n",
    "    print(\"Pruned feature matrix shapes:\")\n",
    "    print(f\"  Train: {X_train_model.shape}\")\n",
    "    print(f\"  Val:   {X_val_model.shape}\")\n",
    "    print(f\"  Test:  {X_test_model.shape}\")\n",
    "else:\n",
    "    X_train_model = X_train\n",
    "    X_val_model = X_val\n",
    "    X_test_model = X_test\n",
    "    feature_names_model = feature_names\n",
    "    print(\"Using all features\")\n",
    "\n",
    "# Recreate datasets with pruned features\n",
    "train_ds = NumpyDataset(X_train_model, y_train)\n",
    "val_ds = NumpyDataset(X_val_model, y_val)\n",
    "test_ds = NumpyDataset(X_test_model, y_test)\n",
    "\n",
    "# Recreate dataloaders\n",
    "# if not use_undersampling:\n",
    "#    class_sample_counts = np.array([(y_train == 0).sum(), (y_train == 1).sum()])\n",
    "#    weights = 1.0 / np.clip(class_sample_counts, 1, None)\n",
    "#    sample_weights = weights[y_train]\n",
    "#    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "#        sample_weights, num_samples=len(sample_weights), replacement=True\n",
    "#    )\n",
    "#    train_loader = DataLoader(\n",
    "#        train_ds, batch_size=batch_size, sampler=sampler, drop_last=False\n",
    "#    )\n",
    "# else:\n",
    "#    train_loader = DataLoader(\n",
    "#        train_ds, batch_size=batch_size, shuffle=True, drop_last=False\n",
    "#    )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798f1a9",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved MLP with:\n",
    "    - Residual connections for better gradient flow\n",
    "    - Layer-wise dropout increasing with depth\n",
    "    - Batch normalization for stable training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, layers, dropout=0.2, use_residual=True):\n",
    "        super().__init__()\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "        # Input projection\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(in_features, layers[0]), nn.BatchNorm1d(layers[0]), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Hidden layers with optional residual connections\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            in_dim = layers[i]\n",
    "            out_dim = layers[i + 1]\n",
    "\n",
    "            # Increase dropout in deeper layers\n",
    "            layer_dropout = dropout * (1 + i * 0.15)\n",
    "            layer_dropout = min(layer_dropout, 0.5)\n",
    "\n",
    "            block = nn.ModuleList(\n",
    "                [\n",
    "                    nn.Linear(in_dim, out_dim),\n",
    "                    nn.BatchNorm1d(out_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(layer_dropout),\n",
    "                ]\n",
    "            )\n",
    "            self.hidden_layers.append(block)\n",
    "\n",
    "            # Residual projection if dimensions don't match\n",
    "            if use_residual and in_dim != out_dim:\n",
    "                self.hidden_layers.append(nn.ModuleList([nn.Linear(in_dim, out_dim)]))\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(layers[-1], 1)\n",
    "\n",
    "        # Store architecture for residuals\n",
    "        self.layer_dims = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        prev_x = x\n",
    "        block_idx = 0\n",
    "        for i in range(len(self.layer_dims) - 1):\n",
    "            block = self.hidden_layers[block_idx]\n",
    "\n",
    "            # Standard forward pass through block\n",
    "            identity = prev_x\n",
    "            for layer in block:\n",
    "                prev_x = layer(prev_x)\n",
    "\n",
    "            # Add residual connection if dimensions match\n",
    "            if self.use_residual:\n",
    "                if self.layer_dims[i] == self.layer_dims[i + 1]:\n",
    "                    prev_x = prev_x + identity\n",
    "                else:\n",
    "                    # Use projection for dimension mismatch\n",
    "                    block_idx += 1\n",
    "                    if block_idx < len(self.hidden_layers):\n",
    "                        projection = self.hidden_layers[block_idx][0]\n",
    "                        prev_x = prev_x + projection(identity)\n",
    "\n",
    "            block_idx += 1\n",
    "\n",
    "        return self.output(prev_x)\n",
    "\n",
    "\n",
    "hidden_layers = [32, 16]\n",
    "\n",
    "dropout = 0.3\n",
    "lr = 1e-3  # Standard Adam LR\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Create improved model\n",
    "model = ImprovedMLP(\n",
    "    in_features=X_train_model.shape[1],\n",
    "    layers=hidden_layers,\n",
    "    dropout=dropout,\n",
    "    use_residual=True,\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"Model architecture\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(model)\n",
    "print(\"\\nParameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1024**2:.2f} MB (float32)\")\n",
    "print(f\"{'=' * 70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89630e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41bdce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-metric configuration\n",
    "primary_metric = \"f1\"  # options: 'auc','f1','ap','balanced_accuracy','mcc'\n",
    "optimize_threshold = True\n",
    "threshold_opt_metric = \"f1\"  # options: 'f1', 'recall', 'precision'\n",
    "target_recall = 0.99  # Target recall to guarantee (only used if threshold_opt_metric='recall')\n",
    "min_improvement = 1e-4  # required relative improvement for early stopping reset\n",
    "use_focal = True\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, decision_threshold=0.5):\n",
    "    model.eval()\n",
    "    logits_list = []\n",
    "    y_list = []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        logits = model(xb)\n",
    "        logits_list.append(logits.cpu())\n",
    "        y_list.append(yb.cpu())\n",
    "    logits = torch.cat(logits_list).squeeze(1)\n",
    "    y_true = torch.cat(y_list).squeeze(1)\n",
    "    probs = torch.sigmoid(logits).numpy()\n",
    "    y_np = y_true.numpy().astype(int)\n",
    "    preds = (probs >= decision_threshold).astype(int)\n",
    "    cm = confusion_matrix(y_np, preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_np, probs)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "    try:\n",
    "        ap = average_precision_score(y_np, probs)\n",
    "    except ValueError:\n",
    "        ap = float(\"nan\")\n",
    "    try:\n",
    "        bal_acc = balanced_accuracy_score(y_np, preds)\n",
    "    except Exception:\n",
    "        bal_acc = float(\"nan\")\n",
    "    try:\n",
    "        f1 = f1_score(y_np, preds, zero_division=0)\n",
    "    except Exception:\n",
    "        f1 = float(\"nan\")\n",
    "    try:\n",
    "        mcc = matthews_corrcoef(y_np, preds)\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "    return dict(\n",
    "        probs=probs,\n",
    "        y=y_np,\n",
    "        preds=preds,\n",
    "        cm=cm,\n",
    "        auc=auc,\n",
    "        ap=ap,\n",
    "        bal_acc=bal_acc,\n",
    "        f1=f1,\n",
    "        mcc=mcc,\n",
    "    )\n",
    "\n",
    "def find_threshold_for_target_recall(y_true, probs, target_recall=0.95):\n",
    "    \"\"\"\n",
    "    Find the threshold that achieves at least the target recall.\n",
    "    Returns the threshold and actual achieved recall.\n",
    "    \"\"\"\n",
    "    # Sort by probability (descending) to efficiently compute recall at different thresholds\n",
    "    sorted_indices = np.argsort(-probs)\n",
    "    sorted_y = y_true[sorted_indices]\n",
    "    \n",
    "    n_positive = y_true.sum()\n",
    "    if n_positive == 0:\n",
    "        return 0.5, 0.0\n",
    "    \n",
    "    # Find minimum threshold that gives desired recall\n",
    "    target_tp = int(np.ceil(target_recall * n_positive))\n",
    "    \n",
    "    # Count true positives as we lower threshold\n",
    "    cumsum_tp = np.cumsum(sorted_y)\n",
    "    \n",
    "    # Find first position where we have enough true positives\n",
    "    threshold_idx = np.searchsorted(cumsum_tp, target_tp)\n",
    "    \n",
    "    if threshold_idx >= len(probs):\n",
    "        # Need all predictions to be positive\n",
    "        threshold = probs.min() - 0.01\n",
    "        actual_recall = 1.0\n",
    "    else:\n",
    "        threshold = probs[sorted_indices[threshold_idx]]\n",
    "        actual_recall = cumsum_tp[threshold_idx] / n_positive\n",
    "    \n",
    "    return threshold, actual_recall\n",
    "\n",
    "# Optional Focal Loss wrapper\n",
    "class FocalBCEWithLogits(nn.Module):\n",
    "    def __init__(self, pos_weight=None, gamma=5.0, label_smoothing=0.01):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction=\"none\")\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Apply label smoothing: 0 -> epsilon, 1 -> 1-epsilon\n",
    "        if self.label_smoothing > 0:\n",
    "            targets = targets * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "\n",
    "        bce_loss = self.bce(logits, targets)\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(logits)\n",
    "            pt = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_factor = (1 - pt) ** self.gamma\n",
    "        return (focal_factor * bce_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training with improvements\n",
    "criterion = (\n",
    "    nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    if use_weighted_loss\n",
    "    else nn.BCEWithLogitsLoss()\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Learning rate scheduler - reduce LR when metric plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",  # maximize metric\n",
    "    factor=0.5,\n",
    "    patience=15,\n",
    "    # verbose=True,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_f1\": [],\n",
    "    \"val_auc\": [],\n",
    "    \"val_recall\": [],\n",
    "    \"val_threshold\": [],\n",
    "    \"learning_rate\": [],\n",
    "}\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_state = None\n",
    "best_threshold = 0.5\n",
    "no_improve = 0\n",
    "\n",
    "# Epochs and early stopping\n",
    "epochs = 1000\n",
    "patience = 50\n",
    "\n",
    "print(f\"Training for {epochs} epochs with patience={patience}\")\n",
    "print(f\"Threshold optimization metric: {threshold_opt_metric}\")\n",
    "if threshold_opt_metric == \"recall\":\n",
    "    print(f\"Target recall: {target_recall:.1%}\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Evaluation\n",
    "    val_metrics = evaluate(model, val_loader, device, decision_threshold=0.5)\n",
    "    probs_val = val_metrics[\"probs\"]\n",
    "    y_val_np = val_metrics[\"y\"]\n",
    "\n",
    "    # Threshold optimization based on chosen metric\n",
    "    thr_opt = 0.5\n",
    "    if optimize_threshold and probs_val.size > 0:\n",
    "        if threshold_opt_metric == \"f1\":\n",
    "            prec_curve, rec_curve, thr_pr = precision_recall_curve(y_val_np, probs_val)\n",
    "            f1_curve = (\n",
    "                2 * prec_curve * rec_curve / np.clip(prec_curve + rec_curve, 1e-9, None)\n",
    "            )\n",
    "            max_f1_idx = np.nanargmax(f1_curve)\n",
    "            if max_f1_idx < len(thr_pr):\n",
    "                thr_opt = thr_pr[max_f1_idx]\n",
    "        elif threshold_opt_metric == \"recall\":\n",
    "            thr_opt, actual_recall = find_threshold_for_target_recall(\n",
    "                y_val_np, probs_val, target_recall=target_recall\n",
    "            )\n",
    "            if VERBOSE and epoch % 5 == 0:\n",
    "                print(f\"  Target recall: {target_recall:.3f}, Achieved: {actual_recall:.3f} @ thr={thr_opt:.3f}\")\n",
    "        elif threshold_opt_metric == \"precision\":\n",
    "            prec_curve, rec_curve, thr_pr = precision_recall_curve(y_val_np, probs_val)\n",
    "            max_prec_idx = np.nanargmax(prec_curve)\n",
    "            if max_prec_idx < len(thr_pr):\n",
    "                thr_opt = thr_pr[max_prec_idx]\n",
    "\n",
    "    # Re-evaluate at optimal threshold\n",
    "    val_metrics_thr = evaluate(model, val_loader, device, decision_threshold=thr_opt)\n",
    "\n",
    "    current_f1 = val_metrics_thr[\"f1\"]\n",
    "    current_auc = val_metrics_thr[\"auc\"]\n",
    "    current_recall = val_metrics_thr[\"cm\"][1, 1] / max(val_metrics_thr[\"cm\"][1, 1] + val_metrics_thr[\"cm\"][1, 0], 1) if val_metrics_thr[\"cm\"].shape[0] > 1 else 0\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    # Update scheduler\n",
    "    scheduler.step(current_f1)\n",
    "\n",
    "    # Log history\n",
    "    history[\"train_loss\"].append(epoch_loss)\n",
    "    history[\"val_f1\"].append(current_f1)\n",
    "    history[\"val_auc\"].append(current_auc)\n",
    "    history[\"val_recall\"].append(current_recall)\n",
    "    history[\"val_threshold\"].append(thr_opt)\n",
    "    history[\"learning_rate\"].append(current_lr)\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 5 == 0 or current_f1 > best_f1:\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | Loss={epoch_loss:.4f} | \"\n",
    "            f\"Val: AUC={current_auc:.4f} F1={current_f1:.4f} Recall={current_recall:.4f} @ thr={thr_opt:.3f} | \"\n",
    "            f\"LR={current_lr:.2e}\"\n",
    "        )\n",
    "\n",
    "    # Early stopping\n",
    "    if current_f1 > best_f1 + 1e-4:\n",
    "        best_f1 = current_f1\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        best_threshold = thr_opt\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "print(f\"Training complete. Best Val F1={best_f1:.4f} at threshold={best_threshold:.4f}\")\n",
    "if threshold_opt_metric == \"recall\":\n",
    "    final_val_metrics = evaluate(model, val_loader, device, decision_threshold=best_threshold)\n",
    "    final_recall = final_val_metrics[\"cm\"][1, 1] / max(final_val_metrics[\"cm\"][1, 1] + final_val_metrics[\"cm\"][1, 0], 1)\n",
    "    print(f\"Final validation recall at best threshold: {final_recall:.4f} (target: {target_recall:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eff7cf",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ae046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using best threshold from validation\n",
    "print(f\"Evaluating on test set with threshold={best_threshold:.4f}\")\n",
    "test_metrics = evaluate(model, test_loader, device, decision_threshold=best_threshold)\n",
    "cm = test_metrics[\"cm\"]\n",
    "probs = test_metrics[\"probs\"]\n",
    "y_true = test_metrics[\"y\"]\n",
    "preds = test_metrics[\"preds\"]\n",
    "auc_final = test_metrics[\"auc\"]\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "report = classification_report(y_true, preds, digits=3, zero_division=0)\n",
    "print(report)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_true, preds, average=\"binary\", zero_division=0\n",
    ")\n",
    "print(f\"Precision={precision:.3f} Recall={recall:.3f} F1={f1:.3f} AUC={auc_final:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (preds == y_true).mean()\n",
    "precision = (\n",
    "    ((preds & (y_true == 1)).sum() / max((preds == 1).sum(), 1))\n",
    "    if (preds == 1).any()\n",
    "    else 0.0\n",
    ")\n",
    "recall = (preds & (y_true == 1)).sum() / max((y_true == 1).sum(), 1)\n",
    "specificity = ((preds == 0) & (y_true == 0)).sum() / max((y_true == 0).sum(), 1)\n",
    "balanced_accuracy = 0.5 * (recall + specificity)\n",
    "try:\n",
    "    mcc = matthews_corrcoef(y_true, preds)\n",
    "except Exception:\n",
    "    mcc = float(\"nan\")\n",
    "try:\n",
    "    ap = average_precision_score(y_true, probs)\n",
    "except Exception:\n",
    "    ap = float(\"nan\")\n",
    "try:\n",
    "    brier = brier_score_loss(y_true, probs)\n",
    "except Exception:\n",
    "    brier = float(\"nan\")\n",
    "\n",
    "# Precision-Recall threshold analysis\n",
    "prec_curve, rec_curve, thr_pr = precision_recall_curve(y_true, probs)\n",
    "f1_curve = 2 * prec_curve * rec_curve / np.clip(prec_curve + rec_curve, 1e-9, None)\n",
    "max_f1_idx = np.nanargmax(f1_curve)\n",
    "opt_pr_threshold = (\n",
    "    thr_pr[max_f1_idx - 1] if max_f1_idx > 0 and max_f1_idx - 1 < len(thr_pr) else 0.5\n",
    ")\n",
    "best_f1 = f1_curve[max_f1_idx]\n",
    "\n",
    "# Youden J optimal ROC threshold\n",
    "fpr_curve, tpr_curve, thr_roc = roc_curve(y_true, probs)\n",
    "youden = tpr_curve - fpr_curve\n",
    "j_idx = np.argmax(youden)\n",
    "youden_thr = thr_roc[j_idx]\n",
    "\n",
    "print(\"--- Extended Metrics ---\")\n",
    "print(f\"Accuracy            : {acc:.4f}\")\n",
    "print(f\"Precision (0.5 cut) : {precision:.4f}\")\n",
    "print(f\"Recall (TPR)        : {recall:.4f}\")\n",
    "print(f\"Specificity (TNR)   : {specificity:.4f}\")\n",
    "print(f\"Balanced Accuracy   : {balanced_accuracy:.4f}\")\n",
    "print(f\"MCC                 : {mcc:.4f}\")\n",
    "print(f\"Average Precision   : {ap:.4f}\")\n",
    "print(f\"Brier Score         : {brier:.4f}\")\n",
    "print(f\"Best F1             : {best_f1:.4f} at PR threshold ~ {opt_pr_threshold:.4f}\")\n",
    "print(f\"Youden J threshold  : {youden_thr:.4f}\")\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rec_curve, prec_curve, label=f\"AP={ap:.3f}\")\n",
    "plt.scatter(\n",
    "    rec_curve[max_f1_idx],\n",
    "    prec_curve[max_f1_idx],\n",
    "    marker=\"o\",\n",
    "    color=\"red\",\n",
    "    label=\"Best F1\",\n",
    ")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fpr_curve, tpr_curve, label=f\"AUC={auc_final:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - DNN\")\n",
    "plt.legend()\n",
    "plt.savefig(\"model_output/precision_recall_curve.png\", dpi=300)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f430db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance breakdown by source file\n",
    "print(\"=\" * 70)\n",
    "print(\"PER-FILE PERFORMANCE ON TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for file_idx, fname in enumerate(files):\n",
    "    mask = file_test == file_idx\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "\n",
    "    probs_file = probs[mask]\n",
    "    y_file = y_true[mask]\n",
    "\n",
    "    # Find optimal threshold for THIS file to maximize recall\n",
    "    best_recall = 0\n",
    "    best_thr_recall = best_threshold\n",
    "    for t in np.linspace(0.1, 0.9, 50):\n",
    "        preds_t = (probs_file >= t).astype(int)\n",
    "        cm_t = confusion_matrix(y_file, preds_t)\n",
    "        if cm_t.shape[0] > 1:\n",
    "            recall_t = cm_t[1, 1] / max(cm_t[1, 1] + cm_t[1, 0], 1)\n",
    "            if recall_t > best_recall:\n",
    "                best_recall = recall_t\n",
    "                best_thr_recall = t\n",
    "\n",
    "    preds_file = (probs_file >= best_threshold).astype(int)\n",
    "    preds_file_optimized = (probs_file >= best_thr_recall).astype(int)\n",
    "\n",
    "    cm_file = confusion_matrix(y_file, preds_file)\n",
    "    cm_file_opt = confusion_matrix(y_file, preds_file_optimized)\n",
    "\n",
    "    try:\n",
    "        auc_file = roc_auc_score(y_file, probs_file)\n",
    "    except ValueError:\n",
    "        auc_file = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        f1_file = f1_score(y_file, preds_file, zero_division=0)\n",
    "    except Exception:\n",
    "        f1_file = float(\"nan\")\n",
    "\n",
    "    recall_file = (\n",
    "        cm_file[1, 1] / max(cm_file[1, 1] + cm_file[1, 0], 1)\n",
    "        if cm_file.shape[0] > 1\n",
    "        else 0\n",
    "    )\n",
    "    recall_file_opt = (\n",
    "        cm_file_opt[1, 1] / max(cm_file_opt[1, 1] + cm_file_opt[1, 0], 1)\n",
    "        if cm_file_opt.shape[0] > 1\n",
    "        else 0\n",
    "    )\n",
    "    precision_file = (\n",
    "        cm_file[1, 1] / max(cm_file[1, 1] + cm_file[0, 1], 1)\n",
    "        if cm_file.shape[0] > 1\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{fname.split('/')[-1]}:\")\n",
    "    print(\n",
    "        f\"  Samples: {mask.sum()} (pos={y_file.sum()}, neg={len(y_file) - y_file.sum()})\"\n",
    "    )\n",
    "    print(f\"  AUC: {auc_file:.4f}\")\n",
    "    print(f\"  F1: {f1_file:.4f}\")\n",
    "    print(f\"  Precision: {precision_file:.4f}\")\n",
    "    print(f\"  Recall @ global thr={best_threshold:.3f}: {recall_file:.4f}\")\n",
    "    print(\n",
    "        f\"  Recall @ optimal thr={best_thr_recall:.3f}: {recall_file_opt:.4f} (+{(recall_file_opt - recall_file) * 100:.1f}%)\"\n",
    "    )\n",
    "    print(f\"  Confusion Matrix (global thr):\\n{cm_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.hist(probs[y_true == 1], bins=40, histtype=\"step\", label=\"matched\")\n",
    "plt.hist(probs[y_true == 0], bins=40, histtype=\"step\", label=\"fake\")\n",
    "plt.xlabel(\"Predicted Probability (matched)\")\n",
    "plt.ylabel(\"Tracks\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_output/predicted_probability.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed783f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlOrRd\", cbar=False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_output/confusion_matrix.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7797f42",
   "metadata": {},
   "source": [
    "## Model improvements analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140dd732",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if FULL_TRAINING\n",
    "# Analyze current model's strengths and weaknesses\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL GENERALIZATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Check for overfitting\n",
    "train_metrics = evaluate(model, train_loader, device, decision_threshold=best_threshold)\n",
    "val_metrics = evaluate(model, val_loader, device, decision_threshold=best_threshold)\n",
    "test_metrics_final = evaluate(\n",
    "    model, test_loader, device, decision_threshold=best_threshold\n",
    ")\n",
    "\n",
    "print(\"\\nPerformance across splits:\")\n",
    "print(f\"{'Split':<10} {'AUC':>8} {'F1':>8} {'Precision':>10} {'Recall':>8}\")\n",
    "print(\"-\" * 50)\n",
    "for split_name, metrics in [\n",
    "    (\"Train\", train_metrics),\n",
    "    (\"Val\", val_metrics),\n",
    "    (\"Test\", test_metrics_final),\n",
    "]:\n",
    "    prec = (\n",
    "        metrics[\"cm\"][1, 1] / max(metrics[\"cm\"][1, 1] + metrics[\"cm\"][0, 1], 1)\n",
    "        if metrics[\"cm\"].shape[0] > 1\n",
    "        else 0\n",
    "    )\n",
    "    rec = (\n",
    "        metrics[\"cm\"][1, 1] / max(metrics[\"cm\"][1, 1] + metrics[\"cm\"][1, 0], 1)\n",
    "        if metrics[\"cm\"].shape[0] > 1\n",
    "        else 0\n",
    "    )\n",
    "    print(\n",
    "        f\"{split_name:<10} {metrics['auc']:>8.4f} {metrics['f1']:>8.4f} {prec:>10.4f} {rec:>8.4f}\"\n",
    "    )\n",
    "\n",
    "# Calculate generalization gap\n",
    "auc_gap = train_metrics[\"auc\"] - test_metrics_final[\"auc\"]\n",
    "f1_gap = train_metrics[\"f1\"] - test_metrics_final[\"f1\"]\n",
    "\n",
    "print(f\"\\nGeneralization gap:\")\n",
    "print(\n",
    "    f\"  AUC gap (train-test): {auc_gap:.4f} {'(good)' if auc_gap < 0.01 else '(check overfitting)'}\"\n",
    ")\n",
    "print(\n",
    "    f\"  F1 gap (train-test): {f1_gap:.4f} {'(good)' if f1_gap < 0.01 else '(check overfitting)'}\"\n",
    ")\n",
    "\n",
    "# 2. Analyze misclassifications\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MISCLASSIFICATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get feature values for misclassified samples\n",
    "test_probs = test_metrics_final[\"probs\"]\n",
    "test_y = test_metrics_final[\"y\"]\n",
    "test_preds = (test_probs >= best_threshold).astype(int)\n",
    "\n",
    "false_positives = (test_preds == 1) & (test_y == 0)\n",
    "false_negatives = (test_preds == 0) & (test_y == 1)\n",
    "\n",
    "print(f\"\\nMisclassification breakdown:\")\n",
    "print(\n",
    "    f\"  False Positives: {false_positives.sum()} ({100 * false_positives.mean():.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  False Negatives: {false_negatives.sum()} ({100 * false_negatives.mean():.2f}%)\"\n",
    ")\n",
    "\n",
    "# Analyze probability distribution for misclassified samples\n",
    "if false_positives.any():\n",
    "    fp_probs = test_probs[false_positives]\n",
    "    print(f\"\\nFalse Positive probabilities:\")\n",
    "    print(f\"  Mean: {fp_probs.mean():.3f}, Median: {np.median(fp_probs):.3f}\")\n",
    "    print(f\"  Min: {fp_probs.min():.3f}, Max: {fp_probs.max():.3f}\")\n",
    "\n",
    "if false_negatives.any():\n",
    "    fn_probs = test_probs[false_negatives]\n",
    "    print(f\"\\nFalse Negative probabilities:\")\n",
    "    print(f\"  Mean: {fn_probs.mean():.3f}, Median: {np.median(fn_probs):.3f}\")\n",
    "    print(f\"  Min: {fn_probs.min():.3f}, Max: {fn_probs.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if FULL_TRAINING\n",
    "# Perform k-fold cross-validation to check model stability\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"K-FOLD CROSS-VALIDATION (for robustness check)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_folds = 5\n",
    "kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = {\"auc\": [], \"f1\": [], \"precision\": [], \"recall\": []}\n",
    "\n",
    "# Use a subset for faster CV\n",
    "cv_sample_size = min(500000, len(X_train_model))\n",
    "cv_indices = np.random.choice(len(X_train_model), size=cv_sample_size, replace=False)\n",
    "X_cv = X_train_model[cv_indices]\n",
    "y_cv = y_train[cv_indices]\n",
    "\n",
    "print(f\"Running {n_folds}-fold CV on {cv_sample_size} samples...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_cv, y_cv)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_folds}...\", end=\" \")\n",
    "\n",
    "    X_fold_train, X_fold_val = X_cv[train_idx], X_cv[val_idx]\n",
    "    y_fold_train, y_fold_val = y_cv[train_idx], y_cv[val_idx]\n",
    "\n",
    "    # Create fold model\n",
    "    fold_model = ImprovedMLP(\n",
    "        in_features=X_train_model.shape[1],\n",
    "        layers=hidden_layers,\n",
    "        dropout=dropout,\n",
    "        use_residual=True,\n",
    "    ).to(device)\n",
    "\n",
    "    fold_optimizer = torch.optim.AdamW(\n",
    "        fold_model.parameters(), lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Quick training (fewer epochs for CV)\n",
    "    fold_train_ds = NumpyDataset(X_fold_train, y_fold_train)\n",
    "    fold_val_ds = NumpyDataset(X_fold_val, y_fold_val)\n",
    "\n",
    "    fold_train_loader = DataLoader(fold_train_ds, batch_size=batch_size, shuffle=True)\n",
    "    fold_val_loader = DataLoader(fold_val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_fold_f1 = 0\n",
    "    patience_fold = 10\n",
    "    no_improve_fold = 0\n",
    "\n",
    "    for epoch in range(50):  # Max 50 epochs per fold\n",
    "        fold_model.train()\n",
    "        for xb, yb in fold_train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            fold_optimizer.zero_grad()\n",
    "            out = fold_model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            fold_optimizer.step()\n",
    "\n",
    "        # Check validation\n",
    "        val_metrics_fold = evaluate(\n",
    "            fold_model, fold_val_loader, device, decision_threshold=0.5\n",
    "        )\n",
    "        if val_metrics_fold[\"f1\"] > best_fold_f1:\n",
    "            best_fold_f1 = val_metrics_fold[\"f1\"]\n",
    "            no_improve_fold = 0\n",
    "        else:\n",
    "            no_improve_fold += 1\n",
    "            if no_improve_fold >= patience_fold:\n",
    "                break\n",
    "\n",
    "    # Final evaluation\n",
    "    final_metrics = evaluate(\n",
    "        fold_model, fold_val_loader, device, decision_threshold=best_threshold\n",
    "    )\n",
    "    cv_scores[\"auc\"].append(final_metrics[\"auc\"])\n",
    "    cv_scores[\"f1\"].append(final_metrics[\"f1\"])\n",
    "\n",
    "    prec = final_metrics[\"cm\"][1, 1] / max(\n",
    "        final_metrics[\"cm\"][1, 1] + final_metrics[\"cm\"][0, 1], 1\n",
    "    )\n",
    "    rec = final_metrics[\"cm\"][1, 1] / max(\n",
    "        final_metrics[\"cm\"][1, 1] + final_metrics[\"cm\"][1, 0], 1\n",
    "    )\n",
    "    cv_scores[\"precision\"].append(prec)\n",
    "    cv_scores[\"recall\"].append(rec)\n",
    "\n",
    "    print(f\"AUC={final_metrics['auc']:.4f}, F1={final_metrics['f1']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "for metric_name, scores in cv_scores.items():\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    print(f\"{metric_name.capitalize():>12}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "print(\n",
    "    f\"\\nModel stability: {'Excellent' if np.std(cv_scores['f1']) < 0.01 else 'Good' if np.std(cv_scores['f1']) < 0.02 else 'Needs improvement'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b67c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if FULL_TRAINING\n",
    "# Train an ensemble of models with different initializations\n",
    "print(\"=\" * 70)\n",
    "print(\"ENSEMBLE MODEL TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_ensemble = 3  # Number of models in ensemble\n",
    "ensemble_models = []\n",
    "ensemble_seeds = [42, 123, 456]\n",
    "\n",
    "print(f\"Training {n_ensemble} models with different initializations...\")\n",
    "\n",
    "for i, seed in enumerate(ensemble_seeds):\n",
    "    print(f\"\\n--- Model {i+1}/{n_ensemble} (seed={seed}) ---\")\n",
    "    \n",
    "    # Set seed for this model\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # Create new model\n",
    "    ensemble_model = ImprovedMLP(\n",
    "        in_features=X_train_model.shape[1],\n",
    "        layers=hidden_layers,\n",
    "        dropout=dropout,\n",
    "        use_residual=True\n",
    "    ).to(device)\n",
    "    \n",
    "    ensemble_optimizer = torch.optim.AdamW(\n",
    "        ensemble_model.parameters(), \n",
    "        lr=lr, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Train for fewer epochs\n",
    "    best_ensemble_f1 = 0\n",
    "    best_ensemble_state = None\n",
    "    patience_ensemble = 20\n",
    "    no_improve_ensemble = 0\n",
    "    \n",
    "    for epoch in range(100):  # Max 100 epochs\n",
    "        ensemble_model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb to(device)\n",
    "            ensemble_optimizer.zero_grad()\n",
    "            out = ensemble_model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            ensemble_optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        if epoch % 10 == 0 or epoch > 80:\n",
    "            val_metrics_ens = evaluate(ensemble_model, val_loader, device, decision_threshold=0.5)\n",
    "            \n",
    "            # Optimize threshold\n",
    "            probs_val = val_metrics_ens[\"probs\"]\n",
    "            y_val_np = val_metrics_ens[\"y\"]\n",
    "            prec_curve, rec_curve, thr_pr = precision_recall_curve(y_val_np, probs_val)\n",
    "            f1_curve = 2 * prec_curve * rec_curve / np.clip(prec_curve + rec_curve, 1e-9, None)\n",
    "            max_f1_idx = np.nanargmax(f1_curve)\n",
    "            thr_opt = thr_pr[max_f1_idx] if max_f1_idx < len(thr_pr) else 0.5\n",
    "            \n",
    "            val_metrics_thr = evaluate(ensemble_model, val_loader, device, decision_threshold=thr_opt)\n",
    "            current_f1 = val_metrics_thr[\"f1\"]\n",
    "            \n",
    "            if current_f1 > best_ensemble_f1:\n",
    "                best_ensemble_f1 = current_f1\n",
    "                best_ensemble_state = {k: v.cpu().clone() for k, v in ensemble_model.state_dict().items()}\n",
    "                no_improve_ensemble = 0\n",
    "            else:\n",
    "                no_improve_ensemble += 1\n",
    "                if no_improve_ensemble >= patience_ensemble:\n",
    "                    break\n",
    "    \n",
    "    # Load best state\n",
    "    if best_ensemble_state is not None:\n",
    "        ensemble_model.load_state_dict(best_ensemble_state)\n",
    "    \n",
    "    ensemble_models.append(ensemble_model)\n",
    "    print(f\"  Best val F1: {best_ensemble_f1:.4f}\")\n",
    "\n",
    "# Ensemble prediction on test set\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get predictions from each model\n",
    "ensemble_probs = []\n",
    "for i, ens_model in enumerate(ensemble_models):\n",
    "    ens_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_tensor = torch.from_numpy(X_test_model.astype(np.float32)).to(device)\n",
    "        logits = ens_model(test_tensor)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy().squeeze()\n",
    "        ensemble_probs.append(probs)\n",
    "\n",
    "# Average predictions\n",
    "ensemble_probs_avg = np.mean(ensemble_probs, axis=0)\n",
    "ensemble_preds = (ensemble_probs_avg >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluate ensemble\n",
    "cm_ensemble = confusion_matrix(y_test, ensemble_preds)\n",
    "auc_ensemble = roc_auc_score(y_test, ensemble_probs_avg)\n",
    "f1_ensemble = f1_score(y_test, ensemble_preds, zero_division=0)\n",
    "\n",
    "prec_ensemble = cm_ensemble[1,1] / max(cm_ensemble[1,1] + cm_ensemble[0,1], 1)\n",
    "rec_ensemble = cm_ensemble[1,1] / max(cm_ensemble[1,1] + cm_ensemble[1,0], 1)\n",
    "\n",
    "print(\"\\nEnsemble vs Single Model Performance:\")\n",
    "print(f\"{'Metric':<15} {'Single Model':>15} {'Ensemble':>15} {'Improvement':>15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'AUC':<15} {auc_final:>15.4f} {auc_ensemble:>15.4f} {'+' if auc_ensemble > auc_final else ''}{(auc_ensemble - auc_final)*100:>14.2f}%\")\n",
    "print(f\"{'F1':<15} {f1:>15.4f} {f1_ensemble:>15.4f} {'+' if f1_ensemble > f1 else ''}{(f1_ensemble - f1)*100:>14.2f}%\")\n",
    "print(f\"{'Precision':<15} {precision:>15.4f} {prec_ensemble:>15.4f} {'+' if prec_ensemble > precision else ''}{(prec_ensemble - precision)*100:>14.2f}%\")\n",
    "print(f\"{'Recall':<15} {recall:>15.4f} {rec_ensemble:>15.4f} {'+' if rec_ensemble > recall else ''}{(rec_ensemble - recall)*100:>14.2f}%\")\n",
    "\n",
    "print(f\"\\n{'✓ Ensemble improves generalization!' if f1_ensemble > f1 else 'Single model is already well-optimized'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669375c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if FULL_TRAINING\n",
    "# Analyze which features contribute to misclassifications\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE ANALYSIS FOR MISCLASSIFICATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get feature values for correct vs incorrect predictions\n",
    "correct_preds = test_preds == y_test\n",
    "incorrect_preds = ~correct_preds\n",
    "\n",
    "print(f\"\\nAnalyzing {len(selected_feature_names)} features...\")\n",
    "print(\"\\nFeature statistics (correct vs incorrect predictions):\")\n",
    "print(f\"{'Feature':<25} {'Correct Mean':>15} {'Incorrect Mean':>18} {'Difference':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "significant_features = []\n",
    "for i, feat_name in enumerate(selected_feature_names):\n",
    "    feat_short = feat_name.split(\"_\")[-1]\n",
    "\n",
    "    correct_mean = X_test_model[correct_preds, i].mean()\n",
    "    incorrect_mean = X_test_model[incorrect_preds, i].mean()\n",
    "    diff = abs(correct_mean - incorrect_mean)\n",
    "\n",
    "    print(\n",
    "        f\"{feat_short:<25} {correct_mean:>15.3f} {incorrect_mean:>18.3f} {diff:>12.3f}\"\n",
    "    )\n",
    "\n",
    "    if diff > 0.5:  # Significant difference (in standardized space)\n",
    "        significant_features.append((feat_short, diff))\n",
    "\n",
    "if significant_features:\n",
    "    print(f\"\\n{len(significant_features)} features show significant differences:\")\n",
    "    for feat, diff in sorted(significant_features, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  - {feat}: {diff:.3f}\")\n",
    "    print(\"\\n💡 Consider adding interaction terms between these features\")\n",
    "else:\n",
    "    print(\"\\n✓ Features are well-balanced between correct and incorrect predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if FULL_TRAINING\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL IMPROVEMENT RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate current performance metrics\n",
    "generalization_quality = (\n",
    "    \"Excellent\"\n",
    "    if abs(train_metrics[\"f1\"] - test_metrics_final[\"f1\"]) < 0.01\n",
    "    else \"Good\"\n",
    ")\n",
    "model_size = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\n📊 Current Model Status:\")\n",
    "print(f\"  Performance: F1={f1:.4f}, AUC={auc_final:.4f}\")\n",
    "print(f\"  Generalization: {generalization_quality}\")\n",
    "print(f\"  Model Size: {model_size:,} parameters\")\n",
    "print(\n",
    "    f\"  Features Used: {len(selected_feature_names)}/32 ({100 * len(selected_feature_names) / 32:.0f}%)\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🎯 Recommended Next Steps:\")\n",
    "\n",
    "if abs(train_metrics[\"f1\"] - test_metrics_final[\"f1\"]) > 0.02:\n",
    "    print(\n",
    "        \"  1. ⚠️  Model shows overfitting - increase regularization (dropout, weight_decay)\"\n",
    "    )\n",
    "elif abs(train_metrics[\"f1\"] - test_metrics_final[\"f1\"]) > 0.01:\n",
    "    print(\"  1. ⚠️  Slight overfitting detected - consider slight increase in dropout\")\n",
    "else:\n",
    "    print(\"  1. ✓ Generalization is excellent - model is well-regularized\")\n",
    "\n",
    "if f1 < 0.95:\n",
    "    print(\"  2. 📈 Try ensemble approach to boost F1 score\")\n",
    "    print(\"  3. 🔍 Analyze misclassifications to add targeted features\")\n",
    "else:\n",
    "    print(\"  2. ✓ F1 score is already very high (95%+)\")\n",
    "\n",
    "if len(selected_feature_names) < 15:\n",
    "    print(f\"  3. 💡 Consider adding 2-3 more features from importance ranking\")\n",
    "elif len(selected_feature_names) > 25:\n",
    "    print(f\"  3. 💡 Model might benefit from more aggressive feature pruning\")\n",
    "else:\n",
    "    print(f\"  3. ✓ Feature count ({len(selected_feature_names)}) is well-balanced\")\n",
    "\n",
    "print(\"\\n🚀 Advanced Improvements (if needed):\")\n",
    "print(\"  • Add feature interactions (multiplicative terms)\")\n",
    "print(\"  • Try deeper architecture with more regularization\")\n",
    "print(\"  • Use focal loss with gamma=2-5 for hard examples\")\n",
    "print(\"  • Implement calibration (Platt scaling) for better probabilities\")\n",
    "print(\"  • Try label smoothing (epsilon=0.01) to prevent overconfidence\")\n",
    "\n",
    "print(\"\\n💻 For Production:\")\n",
    "print(\"  • Current model is production-ready if F1 > 0.95\")\n",
    "print(\"  • Consider ensemble of 3-5 models for maximum robustness\")\n",
    "print(\"  • Monitor per-file performance for physics process variations\")\n",
    "print(\"  • Set threshold based on physics requirements (precision vs recall trade-off)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d926e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt format for python evaluation\n",
    "artifact = {\n",
    "    \"state_dict\": {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "    \"model_config\": {\n",
    "        \"in_features\": X_train_model.shape[1],  # Use pruned feature count\n",
    "        \"layers\": hidden_layers,\n",
    "        \"dropout\": dropout,\n",
    "    },\n",
    "    \"scaler\": scaler,\n",
    "    \"scaler_indices\": selected_feature_indices if use_feature_pruning else None,\n",
    "    \"feature_names\": feature_names_model,  # Use pruned feature names\n",
    "    \"original_feature_names\": feature_names,  # Keep original for reference\n",
    "    \"best_threshold\": best_threshold,\n",
    "    \"auc_test\": float(auc_final),\n",
    "    \"use_feature_pruning\": use_feature_pruning,\n",
    "}\n",
    "output_artifact = \"model_output/simplified_DNN.pt\"\n",
    "torch.save(artifact, output_artifact)\n",
    "size_mb = os.path.getsize(output_artifact) / 1024**2\n",
    "print(f\"Saved artifact to {output_artifact} ({size_mb:.2f} MB)\")\n",
    "print(f\"Model uses {len(feature_names_model)}/{len(feature_names)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaler parameters and feature info for CMSSW\n",
    "# IMPORTANT: Save only the scaler params for the features we actually use\n",
    "if use_feature_pruning:\n",
    "    # Extract scaler parameters only for selected features\n",
    "    scaler_mean_pruned = scaler.mean_[selected_feature_indices].tolist()\n",
    "    scaler_scale_pruned = scaler.scale_[selected_feature_indices].tolist()\n",
    "    features_to_save = selected_feature_names\n",
    "else:\n",
    "    scaler_mean_pruned = scaler.mean_.tolist()\n",
    "    scaler_scale_pruned = scaler.scale_.tolist()\n",
    "    features_to_save = feature_names\n",
    "\n",
    "scaler_params = {\n",
    "    \"mean\": scaler_mean_pruned,\n",
    "    \"scale\": scaler_scale_pruned,\n",
    "    \"pruned_features_indices\": selected_feature_indices.tolist()\n",
    "    if use_feature_pruning\n",
    "    else None,\n",
    "    \"feature_names\": features_to_save,\n",
    "    \"threshold\": float(best_threshold),\n",
    "    \"n_features\": len(features_to_save),\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"model_output/simplified_scaler_params.json\", \"w\") as f:\n",
    "    json.dump(scaler_params, f, indent=2)\n",
    "\n",
    "print(\"Saved scaler parameters to model_output/simplified_scaler_params.json\")\n",
    "print(f\"  Features saved: {len(features_to_save)}\")\n",
    "print(f\"  Feature names: {[f.split('_')[-1] for f in features_to_save]}\")\n",
    "\n",
    "# Sanity check: verify dimensions match\n",
    "assert len(scaler_mean_pruned) == len(features_to_save), (\n",
    "    \"Scaler mean dimension mismatch!\"\n",
    ")\n",
    "assert len(scaler_scale_pruned) == len(features_to_save), (\n",
    "    \"Scaler scale dimension mismatch!\"\n",
    ")\n",
    "assert len(features_to_save) == X_train_model.shape[1], (\n",
    "    \"Feature count mismatch with model input!\"\n",
    ")\n",
    "print(\"✓ Scaler parameters match model input dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610923ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export Model to ONNX for CMSSW integration\n",
    "import torch.onnx\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Create dummy input with the PRUNED feature shape (not original X.shape!)\n",
    "dummy_input = torch.randn(1, X_train_model.shape[1], dtype=torch.float32)\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = \"model_output/simplified_dnn.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=11,  # CMSSW typically supports opset 11-14\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    ")\n",
    "\n",
    "print(f\"Model exported to {onnx_path}\")\n",
    "print(f\"  Input shape: (batch_size, {X_train_model.shape[1]})\")\n",
    "print(f\"  Features: {[f.split('_')[-1] for f in feature_names_model]}\")\n",
    "\n",
    "# Verify the exported model\n",
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model is valid!\")\n",
    "\n",
    "# Test inference with ONNX Runtime using PRUNED features\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Test with a few samples from PRUNED test set\n",
    "test_input = X_test_model[:10].astype(np.float32)\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: test_input}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# Compare with PyTorch output\n",
    "with torch.no_grad():\n",
    "    torch_out = model(torch.from_numpy(test_input))\n",
    "    torch_probs = torch.sigmoid(torch_out).numpy()\n",
    "\n",
    "onnx_probs = 1.0 / (1.0 + np.exp(-ort_outputs[0]))  # sigmoid\n",
    "\n",
    "print(\n",
    "    \"Max difference between PyTorch and ONNX:\", np.abs(torch_probs - onnx_probs).max()\n",
    ")\n",
    "print(\"ONNX Runtime inference successful!\")\n",
    "\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_path)\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "# Test 1: Single track\n",
    "print(\"Test 1: Single track\")\n",
    "single_input = X_test_model[:1].astype(np.float32)\n",
    "single_output = ort_session.run(None, {input_name: single_input})\n",
    "print(f\"  Input shape: {single_input.shape}, Output shape: {single_output[0].shape}\")\n",
    "\n",
    "# Test 2: Multiple tracks (batch)\n",
    "print(\"\\nTest 2: Batch of 10 tracks\")\n",
    "batch_input = X_test_model[:10].astype(np.float32)\n",
    "batch_output = ort_session.run(None, {input_name: batch_input})\n",
    "print(f\"  Input shape: {batch_input.shape}, Output shape: {batch_output[0].shape}\")\n",
    "\n",
    "# Test 3: What happens with different batch sizes\n",
    "print(\"\\nTest 3: Various batch sizes\")\n",
    "for batch_size in [1, 5, 100, 1000]:\n",
    "    test_input = X_test_model[:batch_size].astype(np.float32)\n",
    "    test_output = ort_session.run(None, {input_name: test_input})\n",
    "    print(\n",
    "        f\"  Batch size {batch_size}: Input {test_input.shape} -> Output {test_output[0].shape}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ ONNX model handles all batch sizes correctly\")\n",
    "print(\"Note: Empty batches (0 tracks) should be handled in C++ before calling ONNX\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
