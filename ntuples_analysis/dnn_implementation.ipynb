{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support,\n",
    "    balanced_accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    brier_score_loss,\n",
    ")\n",
    "import os\n",
    "\n",
    "os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"11.0.0\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "VERBOSE = False\n",
    "\n",
    "main_branch = \"Events\"\n",
    "tk_branches = [\n",
    "    \"muon_pixel_tracks_p\",\n",
    "    \"muon_pixel_tracks_pt\",\n",
    "    \"muon_pixel_tracks_ptErr\",\n",
    "    \"muon_pixel_tracks_eta\",\n",
    "    \"muon_pixel_tracks_etaErr\",\n",
    "    \"muon_pixel_tracks_phi\",\n",
    "    \"muon_pixel_tracks_phiErr\",\n",
    "    \"muon_pixel_tracks_chi2\",\n",
    "    \"muon_pixel_tracks_normalizedChi2\",\n",
    "    \"muon_pixel_tracks_nPixelHits\",\n",
    "    \"muon_pixel_tracks_nTrkLays\",\n",
    "    \"muon_pixel_tracks_nFoundHits\",\n",
    "    \"muon_pixel_tracks_nLostHits\",\n",
    "    \"muon_pixel_tracks_dsz\",\n",
    "    \"muon_pixel_tracks_dszErr\",\n",
    "    \"muon_pixel_tracks_dxy\",\n",
    "    \"muon_pixel_tracks_dxyErr\",\n",
    "    \"muon_pixel_tracks_dz\",\n",
    "    \"muon_pixel_tracks_dzErr\",\n",
    "    \"muon_pixel_tracks_qoverp\",\n",
    "    \"muon_pixel_tracks_qoverpErr\",\n",
    "    \"muon_pixel_tracks_lambdaErr\",\n",
    "    \"muon_pixel_tracks_matched\",\n",
    "    \"muon_pixel_tracks_duplicate\",\n",
    "    \"muon_pixel_tracks_tpPdgId\",\n",
    "    \"muon_pixel_tracks_tpPt\",\n",
    "    \"muon_pixel_tracks_tpEta\",\n",
    "    \"muon_pixel_tracks_tpPhi\",\n",
    "]\n",
    "gen_branches = [\n",
    "    \"GenPart_pt\",\n",
    "    \"GenPart_eta\",\n",
    "    \"GenPart_phi\",\n",
    "    \"GenPart_mass\",\n",
    "    \"GenPart_pdgId\",\n",
    "    \"GenPart_statusFlags\",  # added to select last-copy muons\n",
    "]\n",
    "\n",
    "l1tkMuon_branches = [\n",
    "    \"L1TkMu_pt\",\n",
    "    \"L1TkMu_eta\",\n",
    "    \"L1TkMu_phi\",\n",
    "]\n",
    "\n",
    "allPixel = False\n",
    "useSpring24 = True\n",
    "\n",
    "# Configuration Parameters\n",
    "filesSelector = [\n",
    "    \"data/ntuples_TTbarCAExtensionFull.root\",\n",
    "    \"data/ntuples_ZMMCAExtensionFull.root\",\n",
    "    \"data/ntuples_WprimeCAExtensionFull.root\",\n",
    "]\n",
    "\n",
    "filesAllPixel = [\n",
    "    \"data/ntuples_TTbarCAExtensionAllPixel.root\",\n",
    "    \"data/ntuples_ZMMCAExtensionAllPixel.root\",\n",
    "    \"data/ntuples_WprimeCAExtensionAllPixel.root\",\n",
    "]\n",
    "\n",
    "if useSpring24:\n",
    "    filesSelector += [\n",
    "        \"data/spring24/ntuplesExtNoHP_BsToMuMuG.root\",\n",
    "        \"data/spring24/ntuplesExtNoHP_DYToLL.root\",\n",
    "        \"data/spring24/ntuplesExtNoHP_TTTo2L2Nu.root\",\n",
    "        \"data/spring24/ntuplesExtNoHP_TTToSemileptonic.root\",\n",
    "    ]\n",
    "\n",
    "    filesAllPixel += [\n",
    "        \"data/spring24/ntuplesAllPixelNoHP_BsToMuMuG.root\",\n",
    "        \"data/spring24/ntuplesAllPixelNoHP_DYToLL.root\",\n",
    "        \"data/spring24/ntuplesAllPixelNoHP_TTTo2L2Nu.root\",\n",
    "        \"data/spring24/ntuplesAllPixelNoHP_TTToSemileptonic.root\",\n",
    "    ]\n",
    "\n",
    "files = filesSelector if not allPixel else filesAllPixel\n",
    "\n",
    "print(files)\n",
    "\n",
    "# ntuples selection\n",
    "arrays = []\n",
    "file_labels = []  # Track which file each event came from\n",
    "for file_idx, f in enumerate(files):\n",
    "    with uproot.open(f) as file:\n",
    "        arrays_f = file[main_branch].arrays(\n",
    "            tk_branches + gen_branches + l1tkMuon_branches\n",
    "        )\n",
    "        n_events = len(arrays_f)\n",
    "        arrays = ak.concatenate([arrays, arrays_f], axis=0)\n",
    "        file_labels.extend([file_idx] * n_events)  # Label events by file\n",
    "    print(f\"Done loading {f} ({n_events} events)\")\n",
    "\n",
    "print(f\"Loaded {len(arrays)} events from {len(files)} files\")\n",
    "file_labels = np.array(file_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6040ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "\"\"\"\n",
    "import random\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\"\"\"\n",
    "use_gpu = True\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Torch version HIP:\", torch.version.hip)\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b0930",
   "metadata": {},
   "source": [
    "## Build Feature Matrix and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4889362",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"muon_pixel_tracks_p\",\n",
    "    \"muon_pixel_tracks_pt\",\n",
    "    \"muon_pixel_tracks_ptErr\",\n",
    "    \"muon_pixel_tracks_eta\",\n",
    "    \"muon_pixel_tracks_etaErr\",\n",
    "    \"muon_pixel_tracks_phi\",\n",
    "    \"muon_pixel_tracks_phiErr\",\n",
    "    \"muon_pixel_tracks_chi2\",\n",
    "    \"muon_pixel_tracks_normalizedChi2\",\n",
    "    \"muon_pixel_tracks_nPixelHits\",\n",
    "    \"muon_pixel_tracks_nTrkLays\",\n",
    "    \"muon_pixel_tracks_nFoundHits\",\n",
    "    \"muon_pixel_tracks_nLostHits\",\n",
    "    \"muon_pixel_tracks_dsz\",\n",
    "    \"muon_pixel_tracks_dszErr\",\n",
    "    \"muon_pixel_tracks_dxy\",\n",
    "    \"muon_pixel_tracks_dxyErr\",\n",
    "    \"muon_pixel_tracks_dz\",\n",
    "    \"muon_pixel_tracks_dzErr\",\n",
    "    \"muon_pixel_tracks_qoverp\",\n",
    "    \"muon_pixel_tracks_qoverpErr\",\n",
    "    \"muon_pixel_tracks_lambdaErr\",\n",
    "]\n",
    "\n",
    "LABEL_FIELD = \"muon_pixel_tracks_matched\"\n",
    "useL1TkMuFeatures = True\n",
    "\n",
    "\n",
    "def wrap_phi(phi):\n",
    "    \"\"\"Wrap phi to [-pi, pi]\"\"\"\n",
    "    return ((phi + np.pi) % (2 * np.pi)) - np.pi\n",
    "\n",
    "\n",
    "def build_dataset(arr, file_labels_in):\n",
    "    feature_names = list(features)\n",
    "    mask = arr[feature_names[0]] >= 0\n",
    "\n",
    "    # Expand file labels from event level to track level\n",
    "    n_tracks_per_event = ak.num(arr[feature_names[0]])\n",
    "    file_labels_jagged = ak.unflatten(\n",
    "        np.repeat(file_labels_in, n_tracks_per_event), n_tracks_per_event\n",
    "    )\n",
    "    file_labels_masked = ak.to_numpy(ak.flatten(file_labels_jagged[mask]))\n",
    "\n",
    "    # Store original kinematics before log transformation\n",
    "    trk_pt_original = arr[\"muon_pixel_tracks_pt\"]\n",
    "    trk_ptErr_original = arr[\"muon_pixel_tracks_ptErr\"]\n",
    "    trk_eta_original = arr[\"muon_pixel_tracks_eta\"]\n",
    "    trk_phi_original = arr[\"muon_pixel_tracks_phi\"]\n",
    "    trk_chi2 = arr[\"muon_pixel_tracks_chi2\"]\n",
    "    trk_nFoundHits = arr[\"muon_pixel_tracks_nFoundHits\"]\n",
    "    trk_nLostHits = arr[\"muon_pixel_tracks_nLostHits\"]\n",
    "    trk_dxy = arr[\"muon_pixel_tracks_dxy\"]\n",
    "    trk_dz = arr[\"muon_pixel_tracks_dz\"]\n",
    "    trk_dxyErr = arr[\"muon_pixel_tracks_dxyErr\"]\n",
    "    trk_dzErr = arr[\"muon_pixel_tracks_dzErr\"]\n",
    "    trk_qoverp_original = arr[\"muon_pixel_tracks_qoverp\"]\n",
    "    trk_qoverpErr_original = arr[\"muon_pixel_tracks_qoverpErr\"]\n",
    "\n",
    "    cols = []\n",
    "    for f in features:\n",
    "        minimum = ak.min(ak.flatten(arr[f][mask]))\n",
    "        maximum = ak.max(ak.flatten(arr[f][mask]))\n",
    "        if f in [\"muon_pixel_tracks_p\", \"muon_pixel_tracks_pt\"] or \"Err\" in f:\n",
    "            print(f\"Feature {f} min {minimum:.2f} max {maximum:.2f} -> log10\")\n",
    "            arr[f] = np.log10(arr[f] + 1e-6)\n",
    "        flat = ak.to_numpy(ak.flatten(arr[f][mask]))\n",
    "        cols.append(flat)\n",
    "\n",
    "    # DERIVED FEATURES - Help model generalize across physics processes\n",
    "    print(\"\\nAdding derived features...\")\n",
    "\n",
    "    # 1. Relative momentum uncertainty (already have this)\n",
    "    sigmaPtOverPt = np.log10(trk_ptErr_original / (trk_pt_original + 1e-6))\n",
    "    cols.append(ak.to_numpy(ak.flatten(sigmaPtOverPt[mask])))\n",
    "    feature_names.append(\"muon_pixel_tracks_sigmaPtOverPt\")\n",
    "\n",
    "    # 2. Hit efficiency: found / (found + lost)\n",
    "    hit_efficiency = trk_nFoundHits / (trk_nFoundHits + trk_nLostHits + 1e-6)\n",
    "    cols.append(ak.to_numpy(ak.flatten(hit_efficiency[mask])))\n",
    "    feature_names.append(\"muon_pixel_tracks_hitEfficiency\")\n",
    "\n",
    "    # 3. Chi2 per hit (quality per measurement)\n",
    "    chi2_per_hit = trk_chi2 / (trk_nFoundHits + 1e-6)\n",
    "    cols.append(ak.to_numpy(ak.flatten(np.log10(chi2_per_hit + 1e-6)[mask])))\n",
    "    feature_names.append(\"muon_pixel_tracks_chi2PerHit\")\n",
    "\n",
    "    # 4. Impact parameter significance (3D)\n",
    "    impact_param_3d = trk_dxy**2 + trk_dz**2\n",
    "    cols.append(ak.to_numpy(ak.flatten(np.log10(impact_param_3d + 1e-6)[mask])))\n",
    "    feature_names.append(\"muon_pixel_tracks_impact3D\")\n",
    "\n",
    "    # 5. Impact parameter significance (normalized by uncertainty)\n",
    "    dxy_significance = trk_dxy / (trk_dxyErr + 1e-6)\n",
    "    dz_significance = trk_dz / (trk_dzErr + 1e-6)\n",
    "    impact_significance_2d = np.sqrt(dxy_significance**2 + dz_significance**2)\n",
    "    cols.append(ak.to_numpy(ak.flatten(np.log10(impact_significance_2d + 1e-6)[mask])))\n",
    "    feature_names.append(\"muon_pixel_tracks_impactSignificance\")\n",
    "\n",
    "    # 6. Relative uncertainties product (captures overall measurement quality)\n",
    "    rel_uncertainty_product = (trk_ptErr_original / trk_pt_original) * (\n",
    "        trk_qoverpErr_original / trk_qoverp_original\n",
    "    )\n",
    "    cols.append(ak.to_numpy(ak.flatten(np.log10(rel_uncertainty_product + 1e-6)[mask])))\n",
    "    feature_names.append(\"muon_pixel_tracks_relUncertaintyProduct\")\n",
    "\n",
    "    if useL1TkMuFeatures:\n",
    "        print(\"\\nComputing L1TkMuon matching features...\")\n",
    "\n",
    "        l1_pt = arr[\"L1TkMu_pt\"]\n",
    "        l1_eta = arr[\"L1TkMu_eta\"]\n",
    "        l1_phi = arr[\"L1TkMu_phi\"]\n",
    "\n",
    "        trk_zip = ak.zip(\n",
    "            {\"pt\": trk_pt_original, \"eta\": trk_eta_original, \"phi\": trk_phi_original}\n",
    "        )\n",
    "        l1_zip = ak.zip({\"pt\": l1_pt, \"eta\": l1_eta, \"phi\": l1_phi})\n",
    "\n",
    "        pairs = ak.cartesian({\"t\": trk_zip, \"l\": l1_zip}, axis=1, nested=True)\n",
    "\n",
    "        deta = pairs.t.eta - pairs.l.eta\n",
    "        dphi = wrap_phi(pairs.t.phi - pairs.l.phi)\n",
    "        dR2 = deta**2 + dphi**2\n",
    "\n",
    "        min_idx = ak.argmin(dR2, axis=2)\n",
    "        dR2_min = ak.firsts(\n",
    "            dR2[ak.local_index(dR2, axis=2) == min_idx[..., None]], axis=2\n",
    "        )\n",
    "        l1_pt_matched = ak.firsts(\n",
    "            pairs.l.pt[ak.local_index(pairs.l.pt, axis=2) == min_idx[..., None]], axis=2\n",
    "        )\n",
    "\n",
    "        dPt_norm = np.abs(trk_pt_original - l1_pt_matched) / (l1_pt_matched + 1e-6)\n",
    "\n",
    "        # L1 pT ratio\n",
    "        pt_ratio = trk_pt_original / (l1_pt_matched + 1e-6)\n",
    "\n",
    "        # Combined matching score (dR and pT)\n",
    "        matching_score = dR2_min * (1 + dPt_norm)\n",
    "\n",
    "        dR2_min = ak.fill_none(dR2_min, 999.0)\n",
    "        dPt_norm = ak.fill_none(dPt_norm, 999.0)\n",
    "        pt_ratio = ak.fill_none(pt_ratio, 0.0)\n",
    "        matching_score = ak.fill_none(matching_score, 999.0)\n",
    "\n",
    "        dR2_min_flat = ak.to_numpy(ak.flatten(dR2_min[mask]))\n",
    "        dPt_norm_flat = ak.to_numpy(ak.flatten(dPt_norm[mask]))\n",
    "        pt_ratio_flat = ak.to_numpy(ak.flatten(pt_ratio[mask]))\n",
    "        matching_score_flat = ak.to_numpy(ak.flatten(matching_score[mask]))\n",
    "\n",
    "        valid_matches = dPt_norm_flat < 999.0\n",
    "\n",
    "        if VERBOSE:\n",
    "            print(\n",
    "                f\"  ΔR2_min: min={dR2_min_flat.min():.4f}, max={dR2_min_flat.max():.4f}, mean={dR2_min_flat.mean():.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  ΔpT_norm: min={dPt_norm_flat.min():.4f}, max={dPt_norm_flat.max():.4f}, mean={dPt_norm_flat.mean():.4f}\"\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"  Valid L1TkMu matches: {valid_matches.sum()}/{len(valid_matches)} ({valid_matches.mean() * 100:.2f}%)\"\n",
    "            )\n",
    "            if valid_matches.any():\n",
    "                print(\n",
    "                    f\"  ΔpT_norm (valid only): mean={dPt_norm_flat[valid_matches].mean():.4f}, \"\n",
    "                    f\"median={np.median(dPt_norm_flat[valid_matches]):.4f}, \"\n",
    "                    f\"95th percentile={np.percentile(dPt_norm_flat[valid_matches], 95):.4f}\"\n",
    "                )\n",
    "\n",
    "        cols.append(np.log10(dR2_min_flat + 1e-6))\n",
    "        cols.append(np.log10(dPt_norm_flat + 1e-6))\n",
    "        cols.append(np.log10(pt_ratio_flat + 1e-6))\n",
    "        cols.append(np.log10(matching_score_flat + 1e-6))\n",
    "\n",
    "        feature_names.append(\"L1TkMu_dR2min\")\n",
    "        feature_names.append(\"L1TkMu_dPtNorm\")\n",
    "        feature_names.append(\"L1TkMu_ptRatio\")\n",
    "        feature_names.append(\"L1TkMu_matchingScore\")\n",
    "\n",
    "    X = np.vstack(cols).T\n",
    "    y = ak.to_numpy(ak.flatten(arr[LABEL_FIELD][mask])).astype(np.int8)\n",
    "\n",
    "    finite = np.isfinite(X).all(axis=1)\n",
    "    if not finite.all():\n",
    "        print(f\"Removing {(~finite).sum()} non-finite samples\")\n",
    "        X = X[finite]\n",
    "        y = y[finite]\n",
    "        file_labels_masked = file_labels_masked[finite]\n",
    "\n",
    "    return X, y, file_labels_masked, feature_names\n",
    "\n",
    "\n",
    "X, y, file_labels_flat, feature_names = build_dataset(arrays, file_labels)\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape, \"Positives:\", y.sum(), f\"({100 * y.mean():.2f}%)\")\n",
    "print(\"Feature order:\", feature_names)\n",
    "\n",
    "# Show distribution across files\n",
    "print(\"\\nSample distribution by file:\")\n",
    "for file_idx, fname in enumerate(files):\n",
    "    n_samples = (file_labels_flat == file_idx).sum()\n",
    "    n_pos = ((file_labels_flat == file_idx) & (y == 1)).sum()\n",
    "    print(\n",
    "        f\"  File {file_idx} ({fname.split('/')[-1]}): {n_samples} samples ({n_pos} positive, {100 * n_pos / max(n_samples, 1):.2f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b55125",
   "metadata": {},
   "source": [
    "## Train/Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe891ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.7  # fraction for train+test\n",
    "val_frac = 0.15   # fraction (of full dataset) reserved for validation (inside train+val)\n",
    "\n",
    "# Create composite stratification label: combine class label + file source\n",
    "# This ensures both class balance AND file representation in each split\n",
    "stratify_label = y * len(files) + file_labels_flat  # Unique label per (class, file) combination\n",
    "\n",
    "# First split train_val vs test\n",
    "X_train_val, X_test, y_train_val, y_test, file_train_val, file_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    file_labels_flat,\n",
    "    train_size=train_frac,\n",
    "    stratify=stratify_label if (y.sum() > 0 and y.sum() < len(y)) else None,\n",
    ")\n",
    "\n",
    "# Derive actual validation fraction relative to train_val portion\n",
    "if val_frac > 0:\n",
    "    rel_val = val_frac / train_frac  # portion of train_val to carve out as validation\n",
    "    stratify_train_val = y_train_val * len(files) + file_train_val\n",
    "    X_train, X_val, y_train, y_val, file_train, file_val = train_test_split(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        file_train_val,\n",
    "        test_size=rel_val,\n",
    "        stratify=stratify_train_val if (y_train_val.sum() > 0 and y_train_val.sum() < len(y_train_val)) else None,\n",
    "    )\n",
    "else:\n",
    "    X_train, y_train, file_train = X_train_val, y_train_val, file_train_val\n",
    "    X_val, y_val, file_val = X_test, y_test, file_test\n",
    "\n",
    "print(\"Train size:\", X_train.shape, \"Val size:\", X_val.shape, \"Test size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine features pre-scaling \n",
    "print(\"Pre-scaling feature stats:\")\n",
    "for i in range(X_train.shape[1]):\n",
    "    print(f\"{feature_names[i].split('_')[-1]}: mean={X_train[:, i].mean():.4f}, std={X_train[:, i].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51555a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features to zero mean and unit variance\n",
    "print(\"\\nPost-scaling feature stats:\")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "for i in range(X_train.shape[1]):\n",
    "    print(f\"{feature_names[i].split('_')[-1]}: mean={X_train[:, i].mean():.4f}, std={X_train[:, i].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e699a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify file distribution in each split\n",
    "print(\"\\nFile Distribution Verification\")\n",
    "for split_name, file_split, y_split in [(\"Train\", file_train, y_train), \n",
    "                                          (\"Val\", file_val, y_val), \n",
    "                                          (\"Test\", file_test, y_test)]:\n",
    "    print(f\"\\n{split_name} set:\")\n",
    "    for file_idx, fname in enumerate(files):\n",
    "        n_samples = (file_split == file_idx).sum()\n",
    "        n_pos = ((file_split == file_idx) & (y_split == 1)).sum()\n",
    "        pct_of_split = 100 * n_samples / len(file_split)\n",
    "        print(f\"  {fname.split('/')[-1]}: {n_samples} samples ({pct_of_split:.1f}% of {split_name.lower()}, {n_pos} pos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461dfdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_undersampling = True  # Keep file-aware undersampling\n",
    "use_weighted_sampler = False\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32)).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "if use_undersampling:\n",
    "    balanced_indices = []\n",
    "    \n",
    "    print(\"\\n=== File-Aware Undersampling (Optimized for Generalization) ===\")\n",
    "    for file_idx, fname in enumerate(files):\n",
    "        file_mask = file_train == file_idx\n",
    "        file_indices = np.where(file_mask)[0]\n",
    "        \n",
    "        pos_mask = y_train[file_mask] == 1\n",
    "        neg_mask = y_train[file_mask] == 0\n",
    "        \n",
    "        pos_indices = file_indices[pos_mask]\n",
    "        neg_indices = file_indices[neg_mask]\n",
    "        \n",
    "        file_imbalance_ratio = len(neg_indices) / max(len(pos_indices), 1)\n",
    "        \n",
    "        # More aggressive on imbalanced files, but keep consistent ratio\n",
    "        if file_imbalance_ratio > 10:\n",
    "            undersample_ratio = 4  # Slightly more data than before\n",
    "        elif file_imbalance_ratio > 5:\n",
    "            undersample_ratio = 5\n",
    "        else:\n",
    "            undersample_ratio = 6  # Slightly less for ZMM to maintain balance\n",
    "        \n",
    "        n_neg_keep = min(len(neg_indices), len(pos_indices) * undersample_ratio)\n",
    "        neg_indices_sampled = np.random.choice(neg_indices, size=n_neg_keep, replace=False)\n",
    "        \n",
    "        file_balanced = np.concatenate([pos_indices, neg_indices_sampled])\n",
    "        balanced_indices.append(file_balanced)\n",
    "        \n",
    "        print(f\"{fname.split('/')[-1]}:\")\n",
    "        print(f\"  Before: pos={len(pos_indices)} neg={len(neg_indices)} (ratio={file_imbalance_ratio:.1f})\")\n",
    "        print(f\"  After:  pos={len(pos_indices)} neg={n_neg_keep} (ratio={n_neg_keep/max(len(pos_indices),1):.1f})\")\n",
    "    \n",
    "    balanced_indices = np.concatenate(balanced_indices)\n",
    "    np.random.shuffle(balanced_indices)\n",
    "    \n",
    "    X_train_balanced = X_train[balanced_indices]\n",
    "    y_train_balanced = y_train[balanced_indices]\n",
    "    \n",
    "    print(f\"\\nTotal: {len(y_train)} -> {len(y_train_balanced)} samples\")\n",
    "    print(f\"  Class balance: pos={y_train_balanced.sum()} ({100*y_train_balanced.mean():.1f}%), neg={len(y_train_balanced)-y_train_balanced.sum()}\")\n",
    "    \n",
    "    train_ds = NumpyDataset(X_train_balanced, y_train_balanced)\n",
    "else:\n",
    "    train_ds = NumpyDataset(X_train, y_train)\n",
    "\n",
    "pos = y_train_balanced.sum() if use_undersampling else y_train.sum()\n",
    "neg = (len(y_train_balanced) if use_undersampling else len(y_train)) - pos\n",
    "if pos == 0:\n",
    "    pos_weight_value = 1.0\n",
    "else:\n",
    "    pos_weight_value = neg / pos\n",
    "\n",
    "# Balanced approach: moderate pos_weight + undersampling\n",
    "pos_weight_multiplier = 3  # Not too aggressive\n",
    "pos_weight = torch.tensor([pos_weight_value * pos_weight_multiplier], dtype=torch.float32, device=device)\n",
    "print(f\"\\nClass counts train: pos={pos} neg={neg} -> pos_weight={pos_weight_value:.3f} (x{pos_weight_multiplier} = {pos_weight.item():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6a60b",
   "metadata": {},
   "source": [
    "## Torch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "val_ds = NumpyDataset(X_val, y_val)\n",
    "test_ds = NumpyDataset(X_test, y_test)\n",
    "\n",
    "if use_weighted_sampler and not use_undersampling:\n",
    "    # inverse frequency sampling to upweight minority\n",
    "    class_sample_counts = np.array([ (y_train==0).sum(), (y_train==1).sum() ])\n",
    "    weights = 1.0 / np.clip(class_sample_counts, 1, None)\n",
    "    sample_weights = weights[y_train]\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, drop_last=False)\n",
    "else:\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"First batch shapes:\", xb.shape, yb.shape)\n",
    "print(\"Class balance train: pos=\", (y_train_balanced if use_undersampling else y_train).sum(), \n",
    "      \"neg=\", len(y_train_balanced if use_undersampling else y_train)-(y_train_balanced if use_undersampling else y_train).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e9a22",
   "metadata": {},
   "source": [
    "## Define MLP Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, layers, dropout=0):\n",
    "        super().__init__()\n",
    "        seq = []\n",
    "        prev = in_features\n",
    "        for h in layers:\n",
    "            seq.append(nn.Linear(prev, h))\n",
    "            seq.append(nn.BatchNorm1d(h))\n",
    "            seq.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                seq.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        seq.append(nn.Linear(prev, 1))\n",
    "        self.net = nn.Sequential(*seq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "hidden_layers = [64,32] #[512, 256, 128, 64, 32, 16]\n",
    "dropout = 0.15  # slightly higher dropout to regularize\n",
    "lr = 2e-3        # smaller LR for stability\n",
    "weight_decay = 1e-4\n",
    "epochs = 1000\n",
    "patience = 50    # more realistic patience now that we use validation\n",
    "\n",
    "model = MLP(in_features=X.shape[1], layers=hidden_layers, dropout=dropout).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a22322",
   "metadata": {},
   "source": [
    "## Training Loop with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-metric configuration\n",
    "primary_metric = \"f1\"  # options: 'auc','f1','ap','balanced_accuracy','mcc'\n",
    "optimize_threshold = True\n",
    "threshold_opt_metric = \"f1\"  # which metric to maximize when choosing threshold\n",
    "min_improvement = 1e-4  # required relative improvement for early stopping reset\n",
    "use_focal = True\n",
    "focal_gamma = 3\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, decision_threshold=0.5):\n",
    "    model.eval()\n",
    "    logits_list = []\n",
    "    y_list = []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        logits = model(xb)\n",
    "        logits_list.append(logits.cpu())\n",
    "        y_list.append(yb.cpu())\n",
    "    logits = torch.cat(logits_list).squeeze(1)\n",
    "    y_true = torch.cat(y_list).squeeze(1)\n",
    "    probs = torch.sigmoid(logits).numpy()\n",
    "    y_np = y_true.numpy().astype(int)\n",
    "    preds = (probs >= decision_threshold).astype(int)\n",
    "    cm = confusion_matrix(y_np, preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_np, probs)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "    try:\n",
    "        ap = average_precision_score(y_np, probs)\n",
    "    except ValueError:\n",
    "        ap = float(\"nan\")\n",
    "    try:\n",
    "        bal_acc = balanced_accuracy_score(y_np, preds)\n",
    "    except Exception:\n",
    "        bal_acc = float(\"nan\")\n",
    "    try:\n",
    "        f1 = f1_score(y_np, preds, zero_division=0)\n",
    "    except Exception:\n",
    "        f1 = float(\"nan\")\n",
    "    try:\n",
    "        mcc = matthews_corrcoef(y_np, preds)\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "    return dict(\n",
    "        probs=probs,\n",
    "        y=y_np,\n",
    "        preds=preds,\n",
    "        cm=cm,\n",
    "        auc=auc,\n",
    "        ap=ap,\n",
    "        bal_acc=bal_acc,\n",
    "        f1=f1,\n",
    "        mcc=mcc,\n",
    "    )\n",
    "\n",
    "# Optional Focal Loss wrapper\n",
    "class FocalBCEWithLogits(nn.Module):\n",
    "    def __init__(self, pos_weight=None, gamma=2.0, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='none')\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        # Apply label smoothing: 0 -> epsilon, 1 -> 1-epsilon\n",
    "        if self.label_smoothing > 0:\n",
    "            targets = targets * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "        \n",
    "        bce_loss = self.bce(logits, targets)\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(logits)\n",
    "            pt = probs*targets + (1-probs)*(1-targets)\n",
    "        focal_factor = (1-pt)**self.gamma\n",
    "        return (focal_factor * bce_loss).mean()\n",
    "\n",
    "criterion = FocalBCEWithLogits(pos_weight=pos_weight, gamma=focal_gamma, label_smoothing=0.1) if use_focal else nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "best_metric = -1.0\n",
    "best_state = None\n",
    "best_threshold = 0.5\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item() * xb.size(0)\n",
    "    epoch_loss = running / len(train_loader.dataset)\n",
    "\n",
    "    # Evaluate on validation set (use threshold=0.5 first)\n",
    "    val_metrics = evaluate(model, val_loader, device, decision_threshold=0.5)\n",
    "    probs_val = val_metrics[\"probs\"]\n",
    "    y_val = val_metrics[\"y\"]\n",
    "\n",
    "    # Threshold search optimized for GENERALIZATION across files\n",
    "    thr_opt = 0.5\n",
    "    if optimize_threshold and probs_val.size > 0:\n",
    "        candidate_thr = np.unique(np.quantile(probs_val, np.linspace(0, 1, 201)))\n",
    "        best_score_local = -1\n",
    "        \n",
    "        for t in candidate_thr:\n",
    "            preds_t = (probs_val >= t).astype(int)\n",
    "            \n",
    "            if threshold_opt_metric == \"f1\":\n",
    "                # Compute F1 per file and minimize variance (for generalization)\n",
    "                f1_per_file = []\n",
    "                for file_idx in range(len(files)):\n",
    "                    file_mask = file_val == file_idx\n",
    "                    if file_mask.sum() > 0:\n",
    "                        y_file = y_val[file_mask]\n",
    "                        preds_file = preds_t[file_mask]\n",
    "                        f1_file = f1_score(y_file, preds_file, zero_division=0)\n",
    "                        f1_per_file.append(f1_file)\n",
    "                \n",
    "                if len(f1_per_file) > 0:\n",
    "                    # Reward high mean F1 AND low variance (generalization)\n",
    "                    mean_f1 = np.mean(f1_per_file)\n",
    "                    std_f1 = np.std(f1_per_file)\n",
    "                    # Penalize high variance\n",
    "                    metric_t = mean_f1 - 0.5 * std_f1  # Adjust weight as needed\n",
    "                else:\n",
    "                    metric_t = f1_score(y_val, preds_t, zero_division=0)\n",
    "            elif threshold_opt_metric == \"balanced_accuracy\":\n",
    "                metric_t = balanced_accuracy_score(y_val, preds_t)\n",
    "            elif threshold_opt_metric == \"mcc\":\n",
    "                try:\n",
    "                    metric_t = matthews_corrcoef(y_val, preds_t)\n",
    "                except Exception:\n",
    "                    metric_t = -1\n",
    "            else:\n",
    "                metric_t = f1_score(y_val, preds_t, zero_division=0)\n",
    "            \n",
    "            if metric_t > best_score_local:\n",
    "                best_score_local = metric_t\n",
    "                thr_opt = t\n",
    "    \n",
    "    if optimize_threshold:\n",
    "        current_threshold = thr_opt\n",
    "    else:\n",
    "        current_threshold = 0.5\n",
    "\n",
    "    # Recompute metrics at chosen threshold\n",
    "    val_metrics_thr = evaluate(model, val_loader, device, decision_threshold=current_threshold)\n",
    "\n",
    "    auc_val = val_metrics_thr[\"auc\"]\n",
    "    ap_val = val_metrics_thr[\"ap\"]\n",
    "    bal_acc_val = val_metrics_thr[\"bal_acc\"]\n",
    "    f1_val = val_metrics_thr[\"f1\"]\n",
    "    mcc_val = val_metrics_thr[\"mcc\"]\n",
    "\n",
    "    metric_map = {\n",
    "        \"auc\": auc_val,\n",
    "        \"f1\": f1_val,\n",
    "        \"ap\": ap_val,\n",
    "        \"balanced_accuracy\": bal_acc_val,\n",
    "        \"mcc\": mcc_val,\n",
    "    }\n",
    "    current_primary = metric_map.get(primary_metric, f1_val)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d} loss={epoch_loss:.4f} AUC_val={auc_val:.4f} AP_val={ap_val:.4f} F1_val={f1_val:.4f} BalAcc_val={bal_acc_val:.4f} MCC_val={mcc_val:.4f} thr={current_threshold:.3f} primary({primary_metric})={current_primary:.4f}\"  # noqa: E501\n",
    "    )\n",
    "\n",
    "    if current_primary > best_metric + min_improvement * max(abs(best_metric), 1.0):\n",
    "        best_metric = current_primary\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        best_threshold = current_threshold\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "print(f\"Best {primary_metric}={best_metric:.4f} at threshold={best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eff7cf",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ae046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using best threshold from validation\n",
    "print(f\"Evaluating on test set with threshold={best_threshold:.4f}\")\n",
    "test_metrics = evaluate(model, test_loader, device, decision_threshold=best_threshold)\n",
    "cm = test_metrics[\"cm\"]\n",
    "probs = test_metrics[\"probs\"]\n",
    "y_true = test_metrics[\"y\"]\n",
    "preds = test_metrics[\"preds\"]\n",
    "auc_final = test_metrics[\"auc\"]\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "report = classification_report(y_true, preds, digits=3, zero_division=0)\n",
    "print(report)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_true, preds, average=\"binary\", zero_division=0\n",
    ")\n",
    "print(f\"Precision={precision:.3f} Recall={recall:.3f} F1={f1:.3f} AUC={auc_final:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ffbbbc",
   "metadata": {},
   "source": [
    "## Extended Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (preds == y_true).mean()\n",
    "precision = (\n",
    "    ((preds & (y_true == 1)).sum() / max((preds == 1).sum(), 1))\n",
    "    if (preds == 1).any()\n",
    "    else 0.0\n",
    ")\n",
    "recall = (preds & (y_true == 1)).sum() / max((y_true == 1).sum(), 1)\n",
    "specificity = ((preds == 0) & (y_true == 0)).sum() / max((y_true == 0).sum(), 1)\n",
    "balanced_accuracy = 0.5 * (recall + specificity)\n",
    "try:\n",
    "    mcc = matthews_corrcoef(y_true, preds)\n",
    "except Exception:\n",
    "    mcc = float(\"nan\")\n",
    "try:\n",
    "    ap = average_precision_score(y_true, probs)\n",
    "except Exception:\n",
    "    ap = float(\"nan\")\n",
    "try:\n",
    "    brier = brier_score_loss(y_true, probs)\n",
    "except Exception:\n",
    "    brier = float(\"nan\")\n",
    "\n",
    "# Precision-Recall threshold analysis\n",
    "prec_curve, rec_curve, thr_pr = precision_recall_curve(y_true, probs)\n",
    "f1_curve = 2 * prec_curve * rec_curve / np.clip(prec_curve + rec_curve, 1e-9, None)\n",
    "max_f1_idx = np.nanargmax(f1_curve)\n",
    "opt_pr_threshold = (\n",
    "    thr_pr[max_f1_idx - 1] if max_f1_idx > 0 and max_f1_idx - 1 < len(thr_pr) else 0.5\n",
    ")\n",
    "best_f1 = f1_curve[max_f1_idx]\n",
    "\n",
    "# Youden J optimal ROC threshold\n",
    "fpr_curve, tpr_curve, thr_roc = roc_curve(y_true, probs)\n",
    "youden = tpr_curve - fpr_curve\n",
    "j_idx = np.argmax(youden)\n",
    "youden_thr = thr_roc[j_idx]\n",
    "\n",
    "print(\"--- Extended Metrics ---\")\n",
    "print(f\"Accuracy            : {acc:.4f}\")\n",
    "print(f\"Precision (0.5 cut) : {precision:.4f}\")\n",
    "print(f\"Recall (TPR)        : {recall:.4f}\")\n",
    "print(f\"Specificity (TNR)   : {specificity:.4f}\")\n",
    "print(f\"Balanced Accuracy   : {balanced_accuracy:.4f}\")\n",
    "print(f\"MCC                 : {mcc:.4f}\")\n",
    "print(f\"Average Precision   : {ap:.4f}\")\n",
    "print(f\"Brier Score         : {brier:.4f}\")\n",
    "print(f\"Best F1             : {best_f1:.4f} at PR threshold ~ {opt_pr_threshold:.4f}\")\n",
    "print(f\"Youden J threshold  : {youden_thr:.4f}\")\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rec_curve, prec_curve, label=f\"AP={ap:.3f}\")\n",
    "plt.scatter(\n",
    "    rec_curve[max_f1_idx],\n",
    "    prec_curve[max_f1_idx],\n",
    "    marker=\"o\",\n",
    "    color=\"red\",\n",
    "    label=\"Best F1\",\n",
    ")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fpr_curve, tpr_curve, label=f\"AUC={auc_final:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - DNN\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31725ff3",
   "metadata": {},
   "source": [
    "## Per-File Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f430db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance breakdown by source file\n",
    "print(\"=\" * 70)\n",
    "print(\"PER-FILE PERFORMANCE ON TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for file_idx, fname in enumerate(files):\n",
    "    mask = file_test == file_idx\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    probs_file = probs[mask]\n",
    "    y_file = y_true[mask]\n",
    "    \n",
    "    # Find optimal threshold for THIS file to maximize recall\n",
    "    best_recall = 0\n",
    "    best_thr_recall = best_threshold\n",
    "    for t in np.linspace(0.1, 0.9, 50):\n",
    "        preds_t = (probs_file >= t).astype(int)\n",
    "        cm_t = confusion_matrix(y_file, preds_t)\n",
    "        if cm_t.shape[0] > 1:\n",
    "            recall_t = cm_t[1, 1] / max(cm_t[1, 1] + cm_t[1, 0], 1)\n",
    "            if recall_t > best_recall:\n",
    "                best_recall = recall_t\n",
    "                best_thr_recall = t\n",
    "    \n",
    "    preds_file = (probs_file >= best_threshold).astype(int)\n",
    "    preds_file_optimized = (probs_file >= best_thr_recall).astype(int)\n",
    "    \n",
    "    cm_file = confusion_matrix(y_file, preds_file)\n",
    "    cm_file_opt = confusion_matrix(y_file, preds_file_optimized)\n",
    "    \n",
    "    try:\n",
    "        auc_file = roc_auc_score(y_file, probs_file)\n",
    "    except ValueError:\n",
    "        auc_file = float(\"nan\")\n",
    "    \n",
    "    try:\n",
    "        f1_file = f1_score(y_file, preds_file, zero_division=0)\n",
    "    except Exception:\n",
    "        f1_file = float(\"nan\")\n",
    "    \n",
    "    recall_file = cm_file[1, 1] / max(cm_file[1, 1] + cm_file[1, 0], 1) if cm_file.shape[0] > 1 else 0\n",
    "    recall_file_opt = cm_file_opt[1, 1] / max(cm_file_opt[1, 1] + cm_file_opt[1, 0], 1) if cm_file_opt.shape[0] > 1 else 0\n",
    "    precision_file = cm_file[1, 1] / max(cm_file[1, 1] + cm_file[0, 1], 1) if cm_file.shape[0] > 1 else 0\n",
    "    \n",
    "    print(f\"\\n{fname.split('/')[-1]}:\")\n",
    "    print(f\"  Samples: {mask.sum()} (pos={y_file.sum()}, neg={len(y_file)-y_file.sum()})\")\n",
    "    print(f\"  AUC: {auc_file:.4f}\")\n",
    "    print(f\"  F1: {f1_file:.4f}\")\n",
    "    print(f\"  Precision: {precision_file:.4f}\")\n",
    "    print(f\"  Recall @ global thr={best_threshold:.3f}: {recall_file:.4f}\")\n",
    "    print(f\"  Recall @ optimal thr={best_thr_recall:.3f}: {recall_file_opt:.4f} (+{(recall_file_opt-recall_file)*100:.1f}%)\")\n",
    "    print(f\"  Confusion Matrix (global thr):\\n{cm_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c835a09",
   "metadata": {},
   "source": [
    "## Probability Distributions Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.hist(probs[y_true == 1], bins=40, histtype=\"step\", label=\"matched\")\n",
    "plt.hist(probs[y_true == 0], bins=40, histtype=\"step\", label=\"fake\")\n",
    "plt.xlabel(\"Predicted Probability (matched)\")\n",
    "plt.ylabel(\"Tracks\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5450b5f",
   "metadata": {},
   "source": [
    "## Confusion Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed783f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlOrRd\", cbar=False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1821f",
   "metadata": {},
   "source": [
    "## Save Model Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d926e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt format for python evaluation\n",
    "artifact = {\n",
    "    \"state_dict\": {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "    \"model_config\": {\n",
    "        \"in_features\": X.shape[1],\n",
    "        \"layers\": hidden_layers,\n",
    "        \"dropout\": dropout,\n",
    "    },\n",
    "    \"scaler\": scaler,\n",
    "    \"feature_names\": features,\n",
    "    \"best_threshold\": best_threshold,\n",
    "    \"auc_test\": float(auc_final),\n",
    "}\n",
    "output_artifact = \"dnn_artifact.pt\"\n",
    "torch.save(artifact, output_artifact)\n",
    "size_mb = os.path.getsize(output_artifact) / 1024**2\n",
    "print(f\"Saved artifact to {output_artifact} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610923ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export Model to ONNX for CMSSW integration\n",
    "import torch.onnx\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Create dummy input with the correct shape\n",
    "dummy_input = torch.randn(1, X.shape[1], dtype=torch.float32)\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = \"muon_pixeltrack_selector.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=11,  # CMSSW typically supports opset 11-14\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Model exported to {onnx_path}\")\n",
    "\n",
    "# Verify the exported model\n",
    "import onnx\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model is valid!\")\n",
    "\n",
    "# Test inference with ONNX Runtime\n",
    "import onnxruntime as ort\n",
    "ort_session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Test with a few samples\n",
    "test_input = X_test[:10].astype(np.float32)\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: test_input}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# Compare with PyTorch output\n",
    "with torch.no_grad():\n",
    "    torch_out = model(torch.from_numpy(test_input))\n",
    "    torch_probs = torch.sigmoid(torch_out).numpy()\n",
    "\n",
    "onnx_probs = 1.0 / (1.0 + np.exp(-ort_outputs[0]))  # sigmoid\n",
    "\n",
    "print(\"Max difference between PyTorch and ONNX:\", np.abs(torch_probs - onnx_probs).max())\n",
    "print(\"ONNX Runtime inference successful!\")\n",
    "\n",
    "# Save scaler parameters and feature info for CMSSW\n",
    "scaler_params = {\n",
    "    \"mean\": scaler.mean_.tolist(),\n",
    "    \"scale\": scaler.scale_.tolist(),\n",
    "    \"feature_names\": features,\n",
    "    \"threshold\": float(best_threshold),\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(\"scaler_params.json\", \"w\") as f:\n",
    "    json.dump(scaler_params, f, indent=2)\n",
    "\n",
    "print(\"Saved scaler parameters to scaler_params.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43245c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test edge cases with ONNX model\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_path)\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "# Test 1: Single track\n",
    "print(\"Test 1: Single track\")\n",
    "single_input = X_test[:1].astype(np.float32)\n",
    "single_output = ort_session.run(None, {input_name: single_input})\n",
    "print(f\"  Input shape: {single_input.shape}, Output shape: {single_output[0].shape}\")\n",
    "\n",
    "# Test 2: Multiple tracks (batch)\n",
    "print(\"\\nTest 2: Batch of 10 tracks\")\n",
    "batch_input = X_test[:10].astype(np.float32)\n",
    "batch_output = ort_session.run(None, {input_name: batch_input})\n",
    "print(f\"  Input shape: {batch_input.shape}, Output shape: {batch_output[0].shape}\")\n",
    "\n",
    "# Test 3: What happens with different batch sizes\n",
    "print(\"\\nTest 3: Various batch sizes\")\n",
    "for batch_size in [1, 5, 100, 1000]:\n",
    "    test_input = X_test[:batch_size].astype(np.float32)\n",
    "    test_output = ort_session.run(None, {input_name: test_input})\n",
    "    print(f\"  Batch size {batch_size}: Input {test_input.shape} -> Output {test_output[0].shape}\")\n",
    "\n",
    "print(\"\\n✓ ONNX model handles all batch sizes correctly\")\n",
    "print(\"Note: Empty batches (0 tracks) should be handled in C++ before calling ONNX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc15ec",
   "metadata": {},
   "source": [
    "## Visualize L1TkMuon Matching Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad47477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the new L1TkMuon features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Get feature indices\n",
    "dR2_idx = features.index(\"L1TkMu_dR2min\")\n",
    "dPt_idx = features.index(\"L1TkMu_dPtNorm\")\n",
    "\n",
    "# ΔR2 distribution\n",
    "ax = axes[0, 0]\n",
    "ax.hist(X[y == 1, dR2_idx], bins=50, range=(0, 0.5), histtype=\"step\", \n",
    "        label=\"Matched tracks\", linewidth=2, color=\"blue\", density=True)\n",
    "ax.hist(X[y == 0, dR2_idx], bins=50, range=(0, 0.5), histtype=\"step\", \n",
    "        label=\"Fake tracks\", linewidth=2, color=\"red\", density=True)\n",
    "ax.set_xlabel(\"ΔR2 to closest L1TkMuon\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"ΔR2 Distribution\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# ΔpT/pT distribution\n",
    "ax = axes[0, 1]\n",
    "ax.hist(X[y == 1, dPt_idx], bins=50, range=(0, 1.0), histtype=\"step\", \n",
    "        label=\"Matched tracks\", linewidth=2, color=\"blue\", density=True)\n",
    "ax.hist(X[y == 0, dPt_idx], bins=50, range=(0, 1.0), histtype=\"step\", \n",
    "        label=\"Fake tracks\", linewidth=2, color=\"red\", density=True)\n",
    "ax.set_xlabel(\"|pT_track - pT_L1| / pT_L1\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Normalized pT Difference\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 2D correlation: ΔR2 vs ΔpT (matched tracks)\n",
    "ax = axes[1, 0]\n",
    "h = ax.hist2d(X[y == 1, dR2_idx], X[y == 1, dPt_idx], \n",
    "              bins=[50, 50], range=[[0, 0.5], [0, 1.0]], \n",
    "              cmap=\"Blues\", norm=plt.matplotlib.colors.LogNorm())\n",
    "plt.colorbar(h[3], ax=ax, label=\"Matched tracks\")\n",
    "ax.set_xlabel(\"ΔR2 to closest L1TkMuon\")\n",
    "ax.set_ylabel(\"|pT_track - pT_L1| / pT_L1\")\n",
    "ax.set_title(\"Matched Tracks: ΔR2 vs ΔpT\")\n",
    "\n",
    "# 2D correlation: ΔR2 vs ΔpT (fake tracks)\n",
    "ax = axes[1, 1]\n",
    "h = ax.hist2d(X[y == 0, dR2_idx], X[y == 0, dPt_idx], \n",
    "              bins=[50, 50], range=[[0, 0.5], [0, 1.0]], \n",
    "              cmap=\"Reds\", norm=plt.matplotlib.colors.LogNorm())\n",
    "plt.colorbar(h[3], ax=ax, label=\"Fake tracks\")\n",
    "ax.set_xlabel(\"ΔR2 to closest L1TkMuon\")\n",
    "ax.set_ylabel(\"|pT_track - pT_L1| / pT_L1\")\n",
    "ax.set_title(\"Fake Tracks: ΔR2 vs ΔpT\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"L1TkMuon Matching Feature Statistics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMatched tracks (n={(y==1).sum()}):\")\n",
    "print(f\"  ΔR2_min:     mean={X[y==1, dR2_idx].mean():.4f}, std={X[y==1, dR2_idx].std():.4f}\")\n",
    "print(f\"  ΔpT_norm:   mean={X[y==1, dPt_idx].mean():.4f}, std={X[y==1, dPt_idx].std():.4f}\")\n",
    "\n",
    "print(f\"\\nFake tracks (n={(y==0).sum()}):\")\n",
    "print(f\"  ΔR2_min:     mean={X[y==0, dR2_idx].mean():.4f}, std={X[y==0, dR2_idx].std():.4f}\")\n",
    "print(f\"  ΔpT_norm:   mean={X[y==0, dPt_idx].mean():.4f}, std={X[y==0, dPt_idx].std():.4f}\")\n",
    "\n",
    "print(\"\\nSeparation power:\")\n",
    "print(f\"  ΔR2_min:     ratio={X[y==0, dR2_idx].mean() / X[y==1, dR2_idx].mean():.2f}x\")\n",
    "print(f\"  ΔpT_norm:   ratio={X[y==0, dPt_idx].mean() / X[y==1, dPt_idx].mean():.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
