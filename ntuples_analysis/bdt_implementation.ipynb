{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba93abb",
   "metadata": {},
   "source": [
    "# Imports and setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cce770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set the output box size for images\n",
    "display(\n",
    "    HTML(\n",
    "        \"<style>.output_png, .output_jpeg, .output_svg {height: 500px; overflow-y: scroll;}</style>\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Branches for muon pixel tracks\n",
    "main_branch = \"Events\"\n",
    "tk_branches = [\n",
    "    \"muon_pixel_tracks_p\",\n",
    "    \"muon_pixel_tracks_pt\",\n",
    "    \"muon_pixel_tracks_ptErr\",\n",
    "    \"muon_pixel_tracks_eta\",\n",
    "    \"muon_pixel_tracks_etaErr\",\n",
    "    \"muon_pixel_tracks_phi\",\n",
    "    \"muon_pixel_tracks_phiErr\",\n",
    "    \"muon_pixel_tracks_chi2\",\n",
    "    \"muon_pixel_tracks_normalizedChi2\",\n",
    "    \"muon_pixel_tracks_nPixelHits\",\n",
    "    \"muon_pixel_tracks_nTrkLays\",\n",
    "    \"muon_pixel_tracks_nFoundHits\",\n",
    "    \"muon_pixel_tracks_nLostHits\",\n",
    "    \"muon_pixel_tracks_dsz\",\n",
    "    \"muon_pixel_tracks_dszErr\",\n",
    "    \"muon_pixel_tracks_dxy\",\n",
    "    \"muon_pixel_tracks_dxyErr\",\n",
    "    \"muon_pixel_tracks_dz\",\n",
    "    \"muon_pixel_tracks_dzErr\",\n",
    "    \"muon_pixel_tracks_qoverp\",\n",
    "    \"muon_pixel_tracks_qoverpErr\",\n",
    "    \"muon_pixel_tracks_lambdaErr\",\n",
    "    \"muon_pixel_tracks_matched\",\n",
    "    \"muon_pixel_tracks_duplicate\",\n",
    "    \"muon_pixel_tracks_tpPdgId\",\n",
    "    \"muon_pixel_tracks_tpPt\",\n",
    "    \"muon_pixel_tracks_tpEta\",\n",
    "    \"muon_pixel_tracks_tpPhi\",\n",
    "]\n",
    "gen_branches = [\n",
    "    \"GenPart_pt\",\n",
    "    \"GenPart_eta\",\n",
    "    \"GenPart_phi\",\n",
    "    \"GenPart_mass\",\n",
    "    \"GenPart_pdgId\",\n",
    "    \"GenPart_statusFlags\",  # added to select last-copy muons\n",
    "]\n",
    "\n",
    "legacy = False\n",
    "allPixel = False\n",
    "\n",
    "filesSelector = [\n",
    "    \"data/ntuples_TTbarCAExtensionFull.root\",\n",
    "    \"data/ntuples_ZMMCAExtensionFull.root\",\n",
    "    \"data/ntuples_WprimeCAExtensionFull.root\",\n",
    "]\n",
    "\n",
    "filesAllPixel = [\n",
    "    \"data/ntuples_TTbarCAExtensionAllPixel.root\",\n",
    "    \"data/ntuples_ZMMCAExtensionAllPixel.root\",\n",
    "    \"data/ntuples_WprimeCAExtensionAllPixel.root\",\n",
    "]\n",
    "\n",
    "files = filesSelector if not allPixel else filesAllPixel\n",
    "\n",
    "if legacy:\n",
    "    for i, f in enumerate(files):\n",
    "        files[i] = f.replace(\"CAExtension\", \"Legacy\")\n",
    "print(files)\n",
    "# ntuples selection\n",
    "arrays = []\n",
    "for f in files:\n",
    "    with uproot.open(f) as file:\n",
    "        arrays_f = file[main_branch].arrays(tk_branches + gen_branches)\n",
    "        arrays = ak.concatenate([arrays, arrays_f], axis=0)\n",
    "print(f\"Loaded {len(arrays)} events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128ab54",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Prepare flattened feature matrix and labels for model (matched=1, fake=0)\n",
    "def prepare_training_data(\n",
    "    selection=None,\n",
    "    train_fraction=0.7,\n",
    "    shuffle=True,\n",
    "    standardize=True,\n",
    "    drop_duplicates=False,\n",
    "    return_mask=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build (X_train, X_test, y_train, y_test) from per-track awkward arrays.\n",
    "    selection: awkward boolean mask (same jagged structure as tracks) or None (use all)\n",
    "    train_fraction: fraction for training (rest for test)\n",
    "    standardize: apply StandardScaler (fit on train, transform both)\n",
    "    drop_duplicates: optionally remove tracks flagged as duplicate\n",
    "    return_mask: if True also return the flattened boolean mask of kept tracks\n",
    "    \"\"\"\n",
    "    # Default selection: keep all tracks\n",
    "    if selection is None:\n",
    "        selection = arrays.muon_pixel_tracks_pt >= 0  # shape: events, nTracks\n",
    "\n",
    "    # Base mask (apply selection)\n",
    "    mask = selection\n",
    "\n",
    "    if drop_duplicates and \"muon_pixel_tracks_duplicate\" in arrays.fields:\n",
    "        mask = mask & (arrays.muon_pixel_tracks_duplicate == 0)\n",
    "\n",
    "    feature_map = {\n",
    "        \"pt\": arrays.muon_pixel_tracks_pt,\n",
    "        \"eta\": arrays.muon_pixel_tracks_eta,\n",
    "        \"phi\": arrays.muon_pixel_tracks_phi,\n",
    "        # \"p\": arrays.muon_pixel_tracks_p,\n",
    "        # \"dxy\": arrays.muon_pixel_tracks_dxy,\n",
    "        # \"dz\": arrays.muon_pixel_tracks_dz,\n",
    "        \"qoverp\": arrays.muon_pixel_tracks_qoverp,\n",
    "        \"qoverpErr\": arrays.muon_pixel_tracks_qoverpErr,\n",
    "        \"dzErr\": arrays.muon_pixel_tracks_dzErr,\n",
    "        \"etaErr\": arrays.muon_pixel_tracks_etaErr,\n",
    "        \"lambdaErr\": arrays.muon_pixel_tracks_lambdaErr,\n",
    "        \"dxyErr\": arrays.muon_pixel_tracks_dxyErr,\n",
    "        \"phiErr\": arrays.muon_pixel_tracks_phiErr,\n",
    "        # \"chi2\": arrays.muon_pixel_tracks_chi2,\n",
    "        \"normalizedChi2\": arrays.muon_pixel_tracks_normalizedChi2,\n",
    "        \"nPixelHits\": arrays.muon_pixel_tracks_nPixelHits,\n",
    "        \"nTrkLays\": arrays.muon_pixel_tracks_nTrkLays,\n",
    "        # \"nFoundHits\": arrays.muon_pixel_tracks_nFoundHits,\n",
    "        # \"nLostHits\": arrays.muon_pixel_tracks_nLostHits,\n",
    "        # \"dsz\": arrays.muon_pixel_tracks_dsz,\n",
    "        \"dszErr\": arrays.muon_pixel_tracks_dszErr,\n",
    "    }\n",
    "\n",
    "    feature_names = list(feature_map.keys())\n",
    "\n",
    "    # Build list of flattened numpy feature columns applying mask\n",
    "    cols = []\n",
    "    for name in feature_names:\n",
    "        data = feature_map[name][mask]\n",
    "        # flatten to 1D numpy\n",
    "        flat = ak.to_numpy(ak.flatten(data))\n",
    "        cols.append(flat)\n",
    "\n",
    "    # Stack into (n_samples, n_features)\n",
    "    X = np.vstack(cols).T  # shape: (n_tracks, n_features)\n",
    "\n",
    "    # Labels\n",
    "    y = ak.to_numpy(ak.flatten(arrays.muon_pixel_tracks_matched[mask])).astype(\n",
    "        np.int8\n",
    "    )  # 1 matched, 0 fake\n",
    "\n",
    "    # Clean: keep only finite rows\n",
    "    finite_mask = np.isfinite(X).all(axis=1)\n",
    "    if not finite_mask.all():\n",
    "        X = X[finite_mask]\n",
    "        y = y[finite_mask]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        train_size=train_fraction,\n",
    "        shuffle=shuffle,\n",
    "        stratify=y if (y.sum() > 0 and y.sum() < len(y)) else None,\n",
    "    )\n",
    "\n",
    "    scaler = None\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    result = {\n",
    "        \"X\": X,  # full feature matrix (after selection, cleaning)\n",
    "        \"y\": y,  # full labels\n",
    "        \"X_train\": X_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"feature_names\": feature_names,\n",
    "        \"scaler\": scaler,\n",
    "    }\n",
    "    if return_mask:\n",
    "        result[\"global_mask_flat\"] = finite_mask  # after masking & finite filter\n",
    "    return result\n",
    "\n",
    "\n",
    "# IMPORTANT: avoid double scaling (set standardize=False here; Pipeline will scale).\n",
    "data_sets = prepare_training_data(selection=None, train_fraction=0.7, standardize=False)\n",
    "X = data_sets[\"X\"]\n",
    "y = data_sets[\"y\"]\n",
    "X_train = data_sets[\"X_train\"]\n",
    "X_test = data_sets[\"X_test\"]\n",
    "y_train = data_sets[\"y_train\"]\n",
    "y_test = data_sets[\"y_test\"]\n",
    "feature_names = data_sets[\"feature_names\"]\n",
    "scaler = data_sets[\"scaler\"]  # should be None now\n",
    "print(\"Prepared training data (raw features, no pre-standardization).\")\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(y_train)\n",
    "ratio = counter[0] / max(counter[1], 1)\n",
    "class_weight = {0: 1.0, 1: ratio}\n",
    "print(\"Class weights:\", class_weight, \"Class balance train:\", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f9e19d",
   "metadata": {},
   "source": [
    "# BDT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb445fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_selection = False\n",
    "\n",
    "if do_selection:\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    dt = DecisionTreeClassifier(class_weight=class_weight)\n",
    "\n",
    "    searchParam = GridSearchCV(\n",
    "        dt,\n",
    "        {\n",
    "            \"max_leaf_nodes\": list(range(2, 20)),\n",
    "            \"min_samples_split\": list(range(2, 20)),\n",
    "            \"min_samples_leaf\": [20, 50, 75, 100],\n",
    "            \"max_depth\": list(range(5, 20)),\n",
    "        },\n",
    "        cv=5,\n",
    "        n_jobs=50,\n",
    "    )\n",
    "    print(searchParam)\n",
    "    searchParam.fit(X_train, y_train)\n",
    "    print(\"Best params:\", searchParam.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b3083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=8, min_samples_leaf=6, min_samples_split=2, class_weight=class_weight)\n",
    "dt.fit(X_train, y_train)\n",
    "print(\n",
    "    f\"Plain DT acc train={dt.score(X_train, y_train):.3f} test={dt.score(X_test, y_test):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c4b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawTree = False\n",
    "\n",
    "if drawTree:\n",
    "    from graphviz import Source\n",
    "    from sklearn.tree import export_graphviz\n",
    "    import os\n",
    "\n",
    "    export_graphviz(\n",
    "        dt,\n",
    "        out_file=os.path.join(\"./bdt_classifier.dot\"),\n",
    "        feature_names=feature_names,\n",
    "        class_names=[\"fake\", \"matched\"],\n",
    "        rounded=True,\n",
    "        filled=True,\n",
    "    )\n",
    "\n",
    "    Source.from_file(os.path.join(\"./bdt_classifier.dot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model_prediction = cross_val_predict(dt, X_test, y_test, cv=5)\n",
    "\n",
    "cm = confusion_matrix(y_test, model_prediction)\n",
    "# Tool to visualise the confusion matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "df_cm = pd.DataFrame(cm)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sn.heatmap(df_cm, annot=True, cmap=\"YlOrRd\", fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall and F1 score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "ps = precision_score(y_test, model_prediction)\n",
    "rc = recall_score(y_test, model_prediction)\n",
    "f1 = f1_score(y_test, model_prediction)\n",
    "print(\"Precision:\", ps)\n",
    "print(\"Recall:\", rc)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model_prediction)\n",
    "\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")  # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel(\"False Positive Rate (Fall-Out)\", fontsize=16)\n",
    "    plt.ylabel(\"True Positive Rate (Recall)\", fontsize=16)\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9baba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline (single scaling) + diagnostics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# DecisionTreeClassifier(max_depth=6,min_samples_leaf=10,class_weight=class_weight)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\n",
    "            \"clf\",\n",
    "            dt,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "proba_test = pipeline.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, proba_test)\n",
    "print(\n",
    "    f\"Pipeline: acc_train={pipeline.score(X_train, y_train):.3f} acc_test={pipeline.score(X_test, y_test):.3f} AUC={auc:.3f}\"\n",
    ")\n",
    "pred_test = pipeline.predict(X_test)\n",
    "\n",
    "unique_pred, counts_pred = np.unique(pred_test, return_counts=True)\n",
    "print(\"Predicted class distribution on test:\", dict(zip(unique_pred, counts_pred)))\n",
    "print(\n",
    "    \"Probability stats test: min={:.4f} max={:.4f} mean={:.4f} var={:.2e}\".format(\n",
    "        proba_test.min(), proba_test.max(), proba_test.mean(), proba_test.var()\n",
    "    )\n",
    ")\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.hist(proba_test[y_test == 1], bins=40, histtype=\"step\", label=\"matched\")\n",
    "plt.hist(proba_test[y_test == 0], bins=40, histtype=\"step\", label=\"fake\")\n",
    "plt.xlabel(\"Prob(matched)\")\n",
    "plt.ylabel(\"Tracks\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pipeline_prob_dist_fixed.png\", dpi=140)\n",
    "\n",
    "# Save artifact\n",
    "import joblib\n",
    "\n",
    "artifact = {\"pipeline\": pipeline, \"feature_names\": feature_names}\n",
    "joblib.dump(artifact, \"bdt_pipeline.pkl\")\n",
    "print(\"Saved bdt_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea12f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional full-data diagnostic to ensure non-degenerate probabilities\n",
    "full_proba = pipeline.predict_proba(X)[:, 1]\n",
    "uniq_vals = np.unique(np.round(full_proba, 5))\n",
    "spread = full_proba.max() - full_proba.min()\n",
    "print(f\"Full prob spread={spread:.5f}, unique rounded values={len(uniq_vals)}\")\n",
    "if spread < 1e-3 or len(uniq_vals) == 1:\n",
    "    print(\n",
    "        \"WARNING: probabilities nearly constant -> model not separating; consider deeper tree or different algorithm.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
